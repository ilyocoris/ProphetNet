{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import hydra\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import load_states_from_checkpoint\n",
    "from data_utils.s2s_dataset import load_jsonl_data, S2S_dataset\n",
    "from model_utils.create_model import create_model, create_gaussian_diffusion\n",
    "from generate import denoised_fn_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake torch distributed\n",
    "from torch import distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "def initialize_distributed():\n",
    "    if not dist.is_initialized():\n",
    "        # Initialize the distributed environment\n",
    "        dist.init_process_group(backend='gloo')  # 'gloo' is suitable for local development\n",
    "\n",
    "# Call the initialization function\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1' \n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '1235'\n",
    "initialize_distributed()\n",
    "\n",
    "# Now you can use distributed functions safely\n",
    "rank = dist.get_rank()\n",
    "print(f\"Rank {rank} reporting in!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0\n",
    "task = \"recipes\"\n",
    "run = \"eb6ah8_d6_c128_lr2e-4_v2\"\n",
    "checkpoint = \"200000\"\n",
    "seed = 92\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# task = \"recipes\"\n",
    "# run = \"eb6_d6_c128_wd01\"\n",
    "# if hydra initialized, clear it\n",
    "if hydra.core.global_hydra.GlobalHydra.instance() is not None:\n",
    "    hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "# hydra.initialize(config_path=f\"experiment_configs/modulus/{task}\")\n",
    "# config = hydra.compose(config_name=f\"{run}.yaml\")\n",
    "hydra.initialize(config_path=f\"confs\")\n",
    "config = hydra.compose(config_name=f\"config.yaml\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# denoise for more steps\n",
    "# config.clip_denoised = True\n",
    "\n",
    "config.skip_sample = False\n",
    "config.diffusion_steps = 200\n",
    "config.exp.seed = seed\n",
    "\n",
    "\n",
    "print(config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.tokenizer.name_or_path)\n",
    "#  set pad token to \"PAD\"\n",
    "# tokenizer.pad_token = \"PAD\"\n",
    "vocab_size = tokenizer.vocab_size\n",
    "eval_model_path = f\"my_output/{task}/{run}/model/model_checkpoint-{checkpoint}\"\n",
    "# eval_model_path = f\"my_output/{task}/{run}/model/ema_0.9999_checkpoint-{checkpoint}\"\n",
    "# config.load_from_emas = True\n",
    "print(\"Load model from: \", eval_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and load it to device\n",
    "diffusion = create_gaussian_diffusion(config)\n",
    "model = create_model(config, vocab_size)\n",
    "model_saved_state = load_states_from_checkpoint(eval_model_path, dist.get_rank())\n",
    "model.load_state_dict(model_saved_state.model_dict)\n",
    "# model.to(device)\n",
    "# sample text from random noise\n",
    "if config.ddim_sample:\n",
    "    sample_fn = (diffusion.ddim_sample_loop)\n",
    "else:\n",
    "    sample_fn = (diffusion.p_sample_loop)\n",
    "# word embedding\n",
    "emb_model = model.word_embedding\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tgt_path = \"data/raw/recipes/dev.tgt\"\n",
    "test_src_path = \"data/raw/recipes/dev.src\"\n",
    "# load them to dict format\n",
    "test_data = []\n",
    "with open(test_src_path, \"r\") as f_src , open(test_tgt_path, \"r\") as f_tgt:\n",
    "    for src, tgt in zip(f_src, f_tgt):\n",
    "        test_data.append({\"src\":src.strip(), \"tgt\":tgt.strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = max(config.batch_size, 100)\n",
    "test_data = test_data[:1000]\n",
    "\n",
    "dev_dataset = S2S_dataset(test_data, tokenizer, config)\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_dataset, batch_size=BATCH_SIZE, \n",
    "    drop_last=False, pin_memory=True, num_workers=config.num_workers, \n",
    "    collate_fn=S2S_dataset.get_collate_fn(config)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 1 sample for each data\n",
    "each_sample_list = []\n",
    "\n",
    "for _, batch in enumerate(tqdm(dev_dataloader)):\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden_states = model.encoder(\n",
    "            input_ids=batch['src_input_ids'].to(device), \n",
    "            attention_mask=batch['src_attention_mask'].to(device),\n",
    "        ).last_hidden_state  # [bs, seq_len, hz]\n",
    "\n",
    "    if config.pred_len:\n",
    "        with torch.no_grad():\n",
    "            length_out = model.get_pred_len(\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                src_masks=batch['src_attention_mask'].to(device),\n",
    "                normalize=True,\n",
    "            )  # [bs, max_pos_len]\n",
    "            pred_lengs = length_out.max(-1)[1]  # [bs,], max return tuple(value, indices)\n",
    "\n",
    "        tgt_attention_mask = []\n",
    "        for len_item in pred_lengs:\n",
    "            tgt_attention_mask.append([1] * len_item + [0] * (max(pred_lengs) - len_item))\n",
    "        tgt_attention_mask = torch.tensor(tgt_attention_mask).long()\n",
    "        \n",
    "        input_shape = (\n",
    "            tgt_attention_mask.shape[0], tgt_attention_mask.shape[1], config.in_channels,\n",
    "        )\n",
    "    else:\n",
    "        pred_lengs, tgt_attention_mask = None, None\n",
    "        input_shape = (\n",
    "            batch['src_input_ids'].shape[0], config.tgt_len, config.in_channels,\n",
    "        )\n",
    "\n",
    "    model_kwargs = {'src_attention_mask': batch['src_attention_mask'].to(device),\n",
    "                    'tgt_attention_mask': tgt_attention_mask,\n",
    "                    'encoder_hidden_states': encoder_hidden_states,}\n",
    "    sample = sample_fn(\n",
    "        model,\n",
    "        input_shape,\n",
    "        clip_denoised=config.clip_denoised,\n",
    "        # \"Freeze\" some parameters for easy recall.\n",
    "        denoised_fn=partial(denoised_fn_round,\n",
    "                            config, emb_model.to(device)),\n",
    "        progress=True,\n",
    "        model_kwargs=model_kwargs,\n",
    "        pred_lengs=pred_lengs,\n",
    "        top_p=-1.0,\n",
    "    )\n",
    "\n",
    "\n",
    "    logits = model.get_logits(sample)  # (bs, seq_len, vocab_size)\n",
    "    sample_id_tensor = torch.argmax(logits, dim=-1)\n",
    "    generations = tokenizer.batch_decode(sample_id_tensor, skip_special_tokens=True)\n",
    "    each_sample_list.extend(generations)\n",
    "    print(generations[:5], end=\"\\n***\")\n",
    "\n",
    "    # print(tokenizer.batch_decode(sample_id_tensor, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create gen folder if it does not exist\n",
    "if not os.path.exists(f\"my_output/{task}/{run}/gen/dev_{checkpoint}\"):\n",
    "    os.makedirs(f\"my_output/{task}/{run}/gen/dev_{checkpoint}\")\n",
    "# save each_sample_list to my_output/recipes/eb6_d6_c128_wd01/gen/dev.gen\n",
    "with open(f\"my_output/{task}/{run}/gen/dev_{checkpoint}/{seed}.gen\", \"w\") as f:\n",
    "    for item in each_sample_list:\n",
    "        f.write(item+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# compute metrixs\n",
    "with open(f\"my_output/{task}/{run}/gen/dev_{checkpoint}/{seed}.gen\", \"r\") as f:\n",
    "    gen = f.readlines()\n",
    "golds = [t[\"tgt\"] for t in test_data[:len(gen)]]\n",
    "rouge = evaluate.load('rouge')\n",
    "scores = rouge.compute(\n",
    "    predictions=gen,\n",
    "    references=golds,\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p, g in zip(gen[:10], golds[:10]):\n",
    "    print(\"P: \", p)\n",
    "    print(\"G: \", g)\n",
    "    print(\"****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_plot(w, title=\"Attention plot\"):\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(w.detach().cpu().numpy(), cmap='viridis')\n",
    "\n",
    "    # Set ticks on the x-axis for every number\n",
    "    ax.set_xticks(range(w.shape[1]))\n",
    "    ax.set_xticklabels(range(0, w.shape[1])) \n",
    "    # make tick labels vertical\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "    # add legend\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    plt.title(title)\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot attentions\n",
    "w = model.transformer_blocks[0].attn2.attention_probs\n",
    "# w = w.mean(0)\n",
    "attention_plot(w.mean(0), title=f\"Attention plot al heads (mean)\")\n",
    "for i in range(w.shape[0]):\n",
    "        attention_plot(w[i], title=f\"Attention plot for head {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print token correspondence between tgt & gen\n",
    "for tgt, gen in zip([d[\"tgt\"].split(\" \") for d in data_piece], each_sample_list):\n",
    "    print(f\"----------\")\n",
    "    i = 0\n",
    "    for t, g in zip(tgt, gen.split(\" \")):\n",
    "        print(f\"{i}: {t} -> {g}\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
