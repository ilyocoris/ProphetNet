
# Output Path
exp:
  seed: 101
  root: ./my_output
  name: eb1_d1_c128_wd01
  dir: null

# Data
data:
  path: data/raw/sequence
  name: reverse_words
tgt_len: 24
max_pos_len: 24

# Training Basics
batch_size: 48
device: cuda
lr_step: 40000
warmup_steps: 4000
total_steps: 60000
lr: 8e-4
weight_decay: 0.1
grad_clip: -1.0
ema_rate: 0.9999
grad_accum: 1
eval_interval: 3000
log_interval: 1000
save_interval: 15000

# Model
model:
  mode: s2s
  pretrain: null

tokenizer:
  from_pretrained: True
  name_or_path: "bert-base-uncased"

encoder:
  # this will overwrite hidden dim to 768
  initialize_from_pretrained: False
  layers: 1
  num_attention_heads: 4
  att_dropout: 0.1
  is_frozen: False # old fix_encoder

denoiser:
  layers: 1
  num_attention_heads: 4
  att_dropout: 0.1

time_channels: 128
in_channels: 128
out_channels: 128
diffusion_steps: 2000

vocab_size: 30522
intermediate_size: 3072
num_attention_heads: 4
hidden_size: 768

# Diffusion
schedule_sampler: uniform  # uniform / xy_uniform / xy3_uniform / loss-second-moment


# *********** Random parameters  ***********

# src_lang: de
# tgt_lang: en

fairseq:
  use_fairseq: False
  real_data: False
  dist_data: False
use_mbert: False  # mt 
use_bpe: False  # mt 
pad_value: 0  # mt
num_workers: 4  # bpe->4

# training params


#*************************************
# something can change
clip_scale: 0.0
use_step_ratio: False
ratio_thre: 0.7
label_smooth: 0.0
scale_embedding: False

continue_train: False
# TODO: what is this? automatic mixed precision
use_AMP: False
grad_penalty: False
loss_aware: False
pred_len: False
length_factor: 0.1
init_weight: False
prediction: False
pred_len_strategy: null  # token_embed / mean_pool

att_strategy: txl #null  # txl / rotary / null
rel_postion: False
position_att: False
time_att: False
infer_self_condition: False
self_condition: False
end_point_scale: 2.0
dropout: 0.1
att_dropout: 0.1

num_samples: 1
ddim_sample: False
skip_timestep: 100

skip_sample: False
gen_timesteps: 20
#*************************************

# These parameters remain unchanged for now
predict_xstart: True
rescale_timesteps: True
resume_checkpoint: True

shared_embeds: False
roformer_timeAtt: False
add_layer_time: False
use_sentence_piece: False
load_encoder: False
predict_x_start: False
load_bart: False
use_kl: False
learn_pos: False

sigma_small: False
learn_sigma: False
rescale_learned_sigmas: False

logits_mode: 1  # 1 / 2
noise_schedule: sqrt
emb_type: random  # pretrain / random

# generate params
clip_denoised: False
load_from_ema: False
load_step: 0

# pretrain params
mask_pro: 0.3
pre_max_len: 512

add_retrieval_sentences: False
retrieval_top_k: 1
