{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import hydra\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import load_states_from_checkpoint\n",
    "from data_utils.s2s_dataset import load_jsonl_data, S2S_dataset\n",
    "from model_utils.create_model import create_model, create_gaussian_diffusion\n",
    "from generate import denoised_fn_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake torch distributed\n",
    "from torch import distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "def initialize_distributed():\n",
    "    if not dist.is_initialized():\n",
    "        # Initialize the distributed environment\n",
    "        dist.init_process_group(backend='gloo')  # 'gloo' is suitable for local development\n",
    "\n",
    "# Call the initialization function\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1' \n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '1235'\n",
    "initialize_distributed()\n",
    "\n",
    "# Now you can use distributed functions safely\n",
    "rank = dist.get_rank()\n",
    "print(f\"Rank {rank} reporting in!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"k2_m5_bos\"\n",
    "run = \"e1_d6_c128_wd01\"\n",
    "# task = \"recipes\"\n",
    "# run = \"eb6_d6_c128_wd01\"\n",
    "# if hydra initialized, clear it\n",
    "if hydra.core.global_hydra.GlobalHydra.instance() is not None:\n",
    "    hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "# hydra.initialize(config_path=f\"experiment_configs/modulus/{task}\")\n",
    "# config = hydra.compose(config_name=f\"{run}.yaml\")\n",
    "hydra.initialize(config_path=f\".\")\n",
    "config = hydra.compose(config_name=f\"config.yaml\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 4\n",
    "print(config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.tokenizer.name_or_path)\n",
    "#  set pad token to \"PAD\"\n",
    "tokenizer.pad_token = \"PAD\"\n",
    "vocab_size = tokenizer.vocab_size\n",
    "eval_model_path = f\"my_output/{task}/{run}/model/model_checkpoint-60000\"\n",
    "print(\"Load model from: \", eval_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and load it to device\n",
    "diffusion = create_gaussian_diffusion(config)\n",
    "model = create_model(config, vocab_size)\n",
    "model_saved_state = load_states_from_checkpoint(eval_model_path, dist.get_rank())\n",
    "model.load_state_dict(model_saved_state.model_dict)\n",
    "# model.to(device)\n",
    "# sample text from random noise\n",
    "if config.ddim_sample:\n",
    "    sample_fn = (diffusion.ddim_sample_loop)\n",
    "else:\n",
    "    sample_fn = (diffusion.p_sample_loop)\n",
    "# word embedding\n",
    "emb_model = model.word_embedding\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tgt_path = \"data/raw/dyck/k2_m5/test.tgt\"\n",
    "test_src_path = \"data/raw/dyck/k2_m5/test.src\"\n",
    "# load them to dict format\n",
    "test_data = []\n",
    "with open(test_src_path, \"r\") as f_src , open(test_tgt_path, \"r\") as f_tgt:\n",
    "    for src, tgt in zip(f_src, f_tgt):\n",
    "        test_data.append({\"src\":src.strip(), \"tgt\":tgt.strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = S2S_dataset(test_data, tokenizer, config)\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_dataset, batch_size=config.batch_size, \n",
    "    drop_last=False, pin_memory=True, num_workers=config.num_workers, \n",
    "    collate_fn=S2S_dataset.get_collate_fn(config)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 1 sample for each data\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "each_sample_list = []\n",
    "\n",
    "for _, batch in enumerate(tqdm(dev_dataloader)):\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden_states = model.encoder(\n",
    "            input_ids=batch['src_input_ids'].to(device), \n",
    "            attention_mask=batch['src_attention_mask'].to(device),\n",
    "        ).last_hidden_state  # [bs, seq_len, hz]\n",
    "\n",
    "    if config.pred_len:\n",
    "        with torch.no_grad():\n",
    "            length_out = model.get_pred_len(\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                src_masks=batch['src_attention_mask'].to(device),\n",
    "                normalize=True,\n",
    "            )  # [bs, max_pos_len]\n",
    "            pred_lengs = length_out.max(-1)[1]  # [bs,], max return tuple(value, indices)\n",
    "\n",
    "        tgt_attention_mask = []\n",
    "        for len_item in pred_lengs:\n",
    "            tgt_attention_mask.append([1] * len_item + [0] * (max(pred_lengs) - len_item))\n",
    "        tgt_attention_mask = torch.tensor(tgt_attention_mask).long()\n",
    "        \n",
    "        input_shape = (\n",
    "            tgt_attention_mask.shape[0], tgt_attention_mask.shape[1], config.in_channels,\n",
    "        )\n",
    "    else:\n",
    "        pred_lengs, tgt_attention_mask = None, None\n",
    "        input_shape = (\n",
    "            batch['src_input_ids'].shape[0], config.tgt_len, config.in_channels,\n",
    "        )\n",
    "\n",
    "    model_kwargs = {'src_attention_mask': batch['src_attention_mask'].to(device),\n",
    "                    'tgt_attention_mask': tgt_attention_mask,\n",
    "                    'encoder_hidden_states': encoder_hidden_states,}\n",
    "    sample = sample_fn(\n",
    "        model,\n",
    "        input_shape,\n",
    "        clip_denoised=config.clip_denoised,\n",
    "        # \"Freeze\" some parameters for easy recall.\n",
    "        denoised_fn=partial(denoised_fn_round,\n",
    "                            config, emb_model.to(device)),\n",
    "        progress=True,\n",
    "        model_kwargs=model_kwargs,\n",
    "        pred_lengs=pred_lengs,\n",
    "        top_p=-1.0,\n",
    "    )\n",
    "\n",
    "\n",
    "    logits = model.get_logits(sample)  # (bs, seq_len, vocab_size)\n",
    "    sample_id_tensor = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    \n",
    "    each_sample_list.extend(tokenizer.batch_decode(sample_id_tensor, skip_special_tokens=True))\n",
    "\n",
    "    # print(tokenizer.batch_decode(sample_id_tensor, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = tokenizer.batch_decode(batch[\"src_input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "correct = 0\n",
    "for source, gen in zip(sources, each_sample_list):\n",
    "    sentence = re.sub(\"MASK\",gen,src)\n",
    "    sentence = re.sub(\"END\",\"\",sentence)\n",
    "    if is_dyck_2(sentence):\n",
    "        correct += 1\n",
    "    else:\n",
    "        print(gen)\n",
    "\n",
    "print(correct/len(each_sample_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(each_sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dyck_2(sentence):\n",
    "    \"\"\" Check if a sentence is dyck 2, with () and []\"\"\"\n",
    "    stack = []\n",
    "    for c in sentence:\n",
    "        if c == '(' or c == '[':\n",
    "            stack.append(c)\n",
    "        elif c == ')' or c == ']':\n",
    "            if len(stack) == 0: #or stack[-1] != '(':\n",
    "                return False\n",
    "            stack.pop()\n",
    "    return len(stack) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dyck_1(s):\n",
    "    stack = []\n",
    "    for c in s:\n",
    "        if c == '[':\n",
    "            stack.append(c)\n",
    "        elif c == ']':\n",
    "            if len(stack) == 0:\n",
    "                return False\n",
    "            else:\n",
    "                stack.pop()\n",
    "    if len(stack) == 0:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "valid_count = 0\n",
    "for data, gen in zip(test_data, each_sample_list):\n",
    "    dyck_string = re.sub(\"M\",gen,data[\"src\"])\n",
    "    if is_dyck_1(dyck_string):\n",
    "        valid_count += 1\n",
    "        print(\"src: \", data[\"src\"])\n",
    "        print(\"tgt: \", data[\"tgt\"])\n",
    "        print(\"gen: \", gen)\n",
    "        print(dyck_string)\n",
    "        print()\n",
    "    # else:\n",
    "    #     print(dyck_string)\n",
    "    #     print(gen)\n",
    "    #     print()\n",
    "print(\"Accuracy: \", valid_count/len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_plot(w, title=\"Attention plot\"):\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(w.detach().cpu().numpy(), cmap='viridis')\n",
    "\n",
    "    # Set ticks on the x-axis for every number\n",
    "    ax.set_xticks(range(w.shape[1]))\n",
    "    ax.set_xticklabels(range(0, w.shape[1])) \n",
    "    # make tick labels vertical\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "    # add legend\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    plt.title(title)\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot attentions\n",
    "w = model.transformer_blocks[0].attn2.attention_probs\n",
    "# w = w.mean(0)\n",
    "attention_plot(w.mean(0), title=f\"Attention plot al heads (mean)\")\n",
    "for i in range(w.shape[0]):\n",
    "        attention_plot(w[i], title=f\"Attention plot for head {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print token correspondence between tgt & gen\n",
    "for tgt, gen in zip([d[\"tgt\"].split(\" \") for d in data_piece], each_sample_list):\n",
    "    print(f\"----------\")\n",
    "    i = 0\n",
    "    for t, g in zip(tgt, gen.split(\" \")):\n",
    "        print(f\"{i}: {t} -> {g}\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
