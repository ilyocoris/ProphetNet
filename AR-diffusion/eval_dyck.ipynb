{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs/hrist0/gcilleru/ProphetNet/AR-diffusion/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import hydra\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import load_states_from_checkpoint\n",
    "from data_utils.s2s_dataset import load_jsonl_data, S2S_dataset\n",
    "from model_utils.create_model import create_model, create_gaussian_diffusion\n",
    "from generate import denoised_fn_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0 reporting in!\n"
     ]
    }
   ],
   "source": [
    "# fake torch distributed\n",
    "from torch import distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "def initialize_distributed():\n",
    "    if not dist.is_initialized():\n",
    "        # Initialize the distributed environment\n",
    "        dist.init_process_group(backend='gloo')  # 'gloo' is suitable for local development\n",
    "\n",
    "# Call the initialization function\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1' \n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '1235'\n",
    "initialize_distributed()\n",
    "\n",
    "# Now you can use distributed functions safely\n",
    "rank = dist.get_rank()\n",
    "print(f\"Rank {rank} reporting in!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'path': 'data/raw/dyck', 'name': 'k2_m5_bos'}, 'tokenizer': {'from_pretrained': False, 'name_or_path': 'data/raw/dyck/tokenizer_dyck2'}, 'exp': {'seed': 101, 'root': './my_output', 'name': 'eb6_d6_c512', 'dir': None}, 'batch_size': 2, 'device': 'cuda', 'lr_step': 40000, 'warmup_steps': 4000, 'total_steps': 200000, 'lr': 0.0008, 'weight_decay': 1e-05, 'grad_clip': -1.0, 'ema_rate': 0.9999, 'grad_accum': 4, 'eval_interval': 500, 'log_interval': 500, 'save_interval': 20000, 'tgt_len': 128, 'max_pos_len': 256, 'model': {'mode': 's2s', 'pretrain': None}, 'encoder': {'initialize_from_pretrained': False, 'layers': 1, 'num_attention_heads': 4, 'att_dropout': 0.1, 'is_frozen': False}, 'denoiser': {'layers': 6, 'num_attention_heads': 4, 'att_dropout': 0.1}, 'time_channels': 128, 'in_channels': 128, 'out_channels': 128, 'diffusion_steps': 2000, 'vocab_size': 30522, 'intermediate_size': 3072, 'hidden_size': 768, 'schedule_sampler': 'uniform', 'fairseq': {'use_fairseq': False, 'real_data': False, 'dist_data': False}, 'use_mbert': False, 'use_bpe': False, 'pad_value': 0, 'num_workers': 4, 'clip_scale': 0.0, 'use_step_ratio': False, 'ratio_thre': 0.7, 'label_smooth': 0.0, 'scale_embedding': False, 'continue_train': False, 'use_AMP': False, 'grad_penalty': False, 'loss_aware': False, 'pred_len': False, 'length_factor': 0.1, 'init_weight': False, 'prediction': False, 'pred_len_strategy': None, 'att_strategy': 'txl', 'rel_postion': False, 'position_att': False, 'time_att': False, 'infer_self_condition': False, 'self_condition': False, 'end_point_scale': 2.0, 'dropout': 0.1, 'att_dropout': 0.1, 'num_samples': 1, 'ddim_sample': False, 'skip_timestep': 100, 'skip_sample': False, 'gen_timesteps': 20, 'predict_xstart': True, 'rescale_timesteps': True, 'resume_checkpoint': True, 'shared_embeds': False, 'roformer_timeAtt': False, 'add_layer_time': False, 'use_sentence_piece': False, 'load_encoder': False, 'predict_x_start': False, 'load_bart': False, 'use_kl': False, 'learn_pos': False, 'sigma_small': False, 'learn_sigma': False, 'rescale_learned_sigmas': False, 'logits_mode': 1, 'noise_schedule': 'sqrt', 'emb_type': 'random', 'clip_denoised': False, 'load_from_ema': False, 'load_step': 0, 'mask_pro': 0.3, 'pre_max_len': 512, 'add_retrieval_sentences': False, 'retrieval_top_k': 1}\n",
      "Load model from:  my_output/k2_m5_bos/e1_d6_c128_wd01/model/model_checkpoint-60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2527288/3351185181.py:10: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  hydra.initialize(config_path=f\"confs\")\n"
     ]
    }
   ],
   "source": [
    "task = \"k2_m5_bos\"\n",
    "run = \"e1_d6_c128_wd01\"\n",
    "# task = \"recipes\"\n",
    "# run = \"eb6_d6_c128_wd01\"\n",
    "# if hydra initialized, clear it\n",
    "if hydra.core.global_hydra.GlobalHydra.instance() is not None:\n",
    "    hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "# hydra.initialize(config_path=f\"experiment_configs/modulus/{task}\")\n",
    "# config = hydra.compose(config_name=f\"{run}.yaml\")\n",
    "hydra.initialize(config_path=f\"confs\")\n",
    "config = hydra.compose(config_name=f\"config_dyck.yaml\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 5\n",
    "print(config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.tokenizer.name_or_path)\n",
    "#  set pad token to \"PAD\"\n",
    "tokenizer.pad_token = \"PAD\"\n",
    "vocab_size = tokenizer.vocab_size\n",
    "eval_model_path = f\"my_output/{task}/{run}/model/model_checkpoint-60000\"\n",
    "print(\"Load model from: \", eval_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:model_utils.create_model:noise_schedule: sqrt\n",
      "INFO:model_utils.create_model:diffusion steps: 2000\n",
      "INFO:model_utils.create_model:betas: [0.01464131 0.00888909 0.00706818 ... 0.35722328 0.55561113 0.999     ]\n",
      "INFO:model_utils.create_model:Diffusion Loss Type: LossType.E2E_MSE\n",
      "INFO:model_utils.create_model:Whether to learn sigma: False\n",
      "INFO:model_utils.create_model:Diffusion predict xstart: True\n",
      "INFO:model_utils.create_model:training mode is: s2s\n",
      "INFO:model_utils.create_model:creating vanilla model with 1 encoder layers and 6 denoiser layers\n",
      "INFO:model_utils.create_model:loading encoder pretrained BERT model False\n",
      "INFO:model_utils.create_model:rescaling timesteps True\n",
      "INFO:model_utils.create_model:using self condition False\n",
      "INFO:model_utils.create_model:learning time position False\n",
      "INFO:model_utils.create_model:fixing encoder False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:model_utils.diffusion_lm:Load random bert encoder with {encoder_cfg.num_hidden_layers} layers.\n",
      "INFO:utils:Reading saved model from my_output/k2_m5_bos/e1_d6_c128_wd01/model/model_checkpoint-60000\n",
      "INFO:utils:model_state_dict keys dict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CrossAttention_Diffusion_LM(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (input_up_proj): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=1536, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  )\n",
       "  (output_down_proj): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1536, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=1536, out_features=128, bias=True)\n",
       "  )\n",
       "  (word_embedding): Embedding(30522, 128)\n",
       "  (position_embeddings): Embedding(256, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=128, out_features=30522, bias=True)\n",
       "  (time_trans): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-5): 6 x BasicTransformerBlock(\n",
       "      (attn1): CrossAttention(\n",
       "        (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (to_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (to_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropatt): Dropout(p=0.1, inplace=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=768, out_features=6144, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (attn2): CrossAttention(\n",
       "        (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (to_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (to_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropatt): Dropout(p=0.1, inplace=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model and load it to device\n",
    "diffusion = create_gaussian_diffusion(config)\n",
    "model = create_model(config, vocab_size)\n",
    "model_saved_state = load_states_from_checkpoint(eval_model_path, dist.get_rank())\n",
    "model.load_state_dict(model_saved_state.model_dict)\n",
    "# model.to(device)\n",
    "# sample text from random noise\n",
    "if config.ddim_sample:\n",
    "    sample_fn = (diffusion.ddim_sample_loop)\n",
    "else:\n",
    "    sample_fn = (diffusion.p_sample_loop)\n",
    "# word embedding\n",
    "emb_model = model.word_embedding\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tgt_path = \"data/raw/dyck/k2_m5_bos/test.tgt\"\n",
    "test_src_path = \"data/raw/dyck/k2_m5_bos/test.src\"\n",
    "# load them to dict format\n",
    "test_data = []\n",
    "with open(test_src_path, \"r\") as f_src , open(test_tgt_path, \"r\") as f_tgt:\n",
    "    for src, tgt in zip(f_src, f_tgt):\n",
    "        test_data.append({\"src\":src.strip(), \"tgt\":tgt.strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = max(config.batch_size, 5)\n",
    "dev_dataset = S2S_dataset(test_data, tokenizer, config)\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_dataset, batch_size=BATCH_SIZE, \n",
    "    drop_last=False, pin_memory=True, num_workers=config.num_workers, \n",
    "    collate_fn=S2S_dataset.get_collate_fn(config)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/956 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************standard sample**************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:25<00:00, 77.44it/s]\n",
      "  0%|          | 1/956 [00:25<6:53:49, 26.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************standard sample**************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 230/2000 [00:02<00:22, 77.17it/s]\n",
      "  0%|          | 1/956 [00:29<7:42:33, 29.06s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 39\u001b[0m\n\u001b[1;32m     32\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     33\u001b[0m         batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc_input_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], config\u001b[38;5;241m.\u001b[39mtgt_len, config\u001b[38;5;241m.\u001b[39min_channels,\n\u001b[1;32m     34\u001b[0m     )\n\u001b[1;32m     36\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc_attention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc_attention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     37\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtgt_attention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: tgt_attention_mask,\n\u001b[1;32m     38\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder_hidden_states\u001b[39m\u001b[38;5;124m'\u001b[39m: encoder_hidden_states,}\n\u001b[0;32m---> 39\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43msample_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_denoised\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# \"Freeze\" some parameters for easy recall.\u001b[39;49;00m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdenoised_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdenoised_fn_round\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_lengs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_lengs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_logits(sample)  \u001b[38;5;66;03m# (bs, seq_len, vocab_size)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m sample_id_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/mnt/hrist0/gcilleru/ProphetNet/AR-diffusion/model_utils/gaussian_diffusion.py:565\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_sample_loop\u001b[0;34m(self, model, shape, noise, clip_denoised, denoised_fn, model_kwargs, device, progress, pred_lengs, top_p)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m**************standard sample**************\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 565\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_sample_loop_progressive(\n\u001b[1;32m    566\u001b[0m             model,\n\u001b[1;32m    567\u001b[0m             shape,\n\u001b[1;32m    568\u001b[0m             noise\u001b[38;5;241m=\u001b[39mnoise,\n\u001b[1;32m    569\u001b[0m             clip_denoised\u001b[38;5;241m=\u001b[39mclip_denoised,\n\u001b[1;32m    570\u001b[0m             denoised_fn\u001b[38;5;241m=\u001b[39mdenoised_fn,\n\u001b[1;32m    571\u001b[0m             model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs,\n\u001b[1;32m    572\u001b[0m             device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    573\u001b[0m             progress\u001b[38;5;241m=\u001b[39mprogress,\n\u001b[1;32m    574\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m    575\u001b[0m         ):\n\u001b[1;32m    576\u001b[0m             final \u001b[38;5;241m=\u001b[39m sample\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/mnt/hrist0/gcilleru/ProphetNet/AR-diffusion/model_utils/gaussian_diffusion.py:683\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_sample_loop_progressive\u001b[0;34m(self, model, shape, noise, clip_denoised, denoised_fn, model_kwargs, device, progress, top_p)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    682\u001b[0m     self_cond \u001b[38;5;241m=\u001b[39m x_start \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39minfer_self_condition \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 683\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mself_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_denoised\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdenoised_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdenoised_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m out\n\u001b[1;32m    694\u001b[0m     text \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/mnt/hrist0/gcilleru/ProphetNet/AR-diffusion/model_utils/gaussian_diffusion.py:608\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_sample\u001b[0;34m(self, model, x, t, self_cond, clip_denoised, denoised_fn, model_kwargs, top_p)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;124;03mSample x_{t-1} from the model at the given timestep.\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;124;03m         - 'pred_xstart': a prediction of x_0.\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;66;03m# out: x_{t-1}\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_mean_variance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mself_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_denoised\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdenoised_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdenoised_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m top_p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m top_p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    618\u001b[0m     noise \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mrandn_like(x)\n",
      "File \u001b[0;32m/mnt/hrist0/gcilleru/ProphetNet/AR-diffusion/model_utils/gaussian_diffusion.py:419\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_mean_variance\u001b[0;34m(self, model, x, t, self_cond, clip_denoised, denoised_fn, model_kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_mean_type \u001b[38;5;129;01min\u001b[39;00m [ModelMeanType\u001b[38;5;241m.\u001b[39mSTART_X, ModelMeanType\u001b[38;5;241m.\u001b[39mEPSILON]:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_mean_type \u001b[38;5;241m==\u001b[39m ModelMeanType\u001b[38;5;241m.\u001b[39mSTART_X:\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;66;03m# Select the nearest embedding by KNN, that is predicting the x_0\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m         pred_xstart \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_xstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m         pred_xstart \u001b[38;5;241m=\u001b[39m process_xstart(\n\u001b[1;32m    422\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_xstart_from_eps(x_t\u001b[38;5;241m=\u001b[39mx, t\u001b[38;5;241m=\u001b[39mt, eps\u001b[38;5;241m=\u001b[39mmodel_output)\n\u001b[1;32m    423\u001b[0m         )\n",
      "File \u001b[0;32m/mnt/hrist0/gcilleru/ProphetNet/AR-diffusion/model_utils/gaussian_diffusion.py:406\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_mean_variance.<locals>.process_xstart\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_xstart\u001b[39m(x):\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m denoised_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mdenoised_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clip_denoised:\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/mnt/hrist0/gcilleru/ProphetNet/AR-diffusion/generate.py:54\u001b[0m, in \u001b[0;36mdenoised_fn_round\u001b[0;34m(config, emb_model, text_emb, t)\u001b[0m\n\u001b[1;32m     50\u001b[0m     text_emb \u001b[38;5;241m=\u001b[39m text_emb\n\u001b[1;32m     52\u001b[0m val, indices \u001b[38;5;241m=\u001b[39m get_efficient_knn(down_proj_emb,\n\u001b[1;32m     53\u001b[0m                                  text_emb\u001b[38;5;241m.\u001b[39mto(down_proj_emb\u001b[38;5;241m.\u001b[39mdevice), dist\u001b[38;5;241m=\u001b[39mdist)\n\u001b[0;32m---> 54\u001b[0m rounded_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# (bs*seq_len,)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m new_embeds \u001b[38;5;241m=\u001b[39m emb_model(rounded_tokens)\u001b[38;5;241m.\u001b[39mview(old_shape)\u001b[38;5;241m.\u001b[39mto(old_device)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_embeds\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# generate 1 sample for each data\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "each_sample_list = []\n",
    "\n",
    "for _, batch in enumerate(tqdm(dev_dataloader)):\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden_states = model.encoder(\n",
    "            input_ids=batch['src_input_ids'].to(device), \n",
    "            attention_mask=batch['src_attention_mask'].to(device),\n",
    "        ).last_hidden_state  # [bs, seq_len, hz]\n",
    "\n",
    "    if config.pred_len:\n",
    "        with torch.no_grad():\n",
    "            length_out = model.get_pred_len(\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                src_masks=batch['src_attention_mask'].to(device),\n",
    "                normalize=True,\n",
    "            )  # [bs, max_pos_len]\n",
    "            pred_lengs = length_out.max(-1)[1]  # [bs,], max return tuple(value, indices)\n",
    "\n",
    "        tgt_attention_mask = []\n",
    "        for len_item in pred_lengs:\n",
    "            tgt_attention_mask.append([1] * len_item + [0] * (max(pred_lengs) - len_item))\n",
    "        tgt_attention_mask = torch.tensor(tgt_attention_mask).long()\n",
    "        \n",
    "        input_shape = (\n",
    "            tgt_attention_mask.shape[0], tgt_attention_mask.shape[1], config.in_channels,\n",
    "        )\n",
    "    else:\n",
    "        pred_lengs, tgt_attention_mask = None, None\n",
    "        input_shape = (\n",
    "            batch['src_input_ids'].shape[0], config.tgt_len, config.in_channels,\n",
    "        )\n",
    "\n",
    "    model_kwargs = {'src_attention_mask': batch['src_attention_mask'].to(device),\n",
    "                    'tgt_attention_mask': tgt_attention_mask,\n",
    "                    'encoder_hidden_states': encoder_hidden_states,}\n",
    "    sample = sample_fn(\n",
    "        model,\n",
    "        input_shape,\n",
    "        clip_denoised=config.clip_denoised,\n",
    "        # \"Freeze\" some parameters for easy recall.\n",
    "        denoised_fn=partial(denoised_fn_round,\n",
    "                            config, emb_model.to(device)),\n",
    "        progress=True,\n",
    "        model_kwargs=model_kwargs,\n",
    "        pred_lengs=pred_lengs,\n",
    "        top_p=-1.0,\n",
    "    )\n",
    "\n",
    "\n",
    "    logits = model.get_logits(sample)  # (bs, seq_len, vocab_size)\n",
    "    sample_id_tensor = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    \n",
    "    each_sample_list.extend(tokenizer.batch_decode(sample_id_tensor, skip_special_tokens=True))\n",
    "\n",
    "    # print(tokenizer.batch_decode(sample_id_tensor, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = tokenizer.batch_decode(batch[\"src_input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS ( ) [ ] ( ) ( ) ( ) [ [ ] [ [  ) ( ) ) ( ( ) ) [ ( ( ) ) ] ] ] [ ( ( ) [ ( [ ] ) ] ( [ ] ) ( [ ] [ ] ) ( ) ( [ [ ] ] ) ( [ ] ) [ ( ) ( ) ] ( [ ( ) ] ( ) ) ) ] ( ( ( [ ] [ ] [ ] [ ] [ [ ] ( ) ] ( ( ) [ ] ) ) ( ( ( ) ) [ ] ( [ ] ) [ ( ) ] ) ( ) ) ( ) ( [ ( [ ] ) ] ( ) [ [ ( ) ( ) ] [ ] ( [ ] ( ) [ ] ( ) ) ] ) [ ] [ ] ) \n",
      "\n",
      "\n",
      "BOS ( ) [ ] ( ) ( ) ( ) [ [ ] [ [ ] ( ) ( ) ) ( ( ) ) [ ( ( ) ) ] ] ] [ ( ( ) [ ( [ ] ) ] ( [ ] ) ( [ ] [ ] ) ( ) ( [ [ ] ] ) ( [ ] ) [ ( ) ( ) ] ( [ ( ) ] ( ) ) ) ] ( ( ( [ ] [ ] [ ] [ ] [ [ ] ( ) ] ( ( ) [ ] ) ) ( ( ( ) ) [ ] ( [ ] ) [ ( ) ] ) ( ) ) ( ) ( [ ( [ ] ) ] ( ) [ [ ( ) ( ) ] [ ] ( [ ] ( ) [ ] ( ) ) ] ) [ ] [ ] ) \n",
      "\n",
      "] (\n",
      "BOS ( ) [ ] ( ) ( ) ( ) [ [ ] [ [  ) ( ) ) ( ( ) ) [ ( ( ) ) ] ] ] [ ( ( ) [ ( [ ] ) ] ( [ ] ) ( [ ] [ ] ) ( ) ( [ [ ] ] ) ( [ ] ) [ ( ) ( ) ] ( [ ( ) ] ( ) ) ) ] ( ( ( [ ] [ ] [ ] [ ] [ [ ] ( ) ] ( ( ) [ ] ) ) ( ( ( ) ) [ ] ( [ ] ) [ ( ) ] ) ( ) ) ( ) ( [ ( [ ] ) ] ( ) [ [ ( ) ( ) ] [ ] ( [ ] ( ) [ ] ( ) ) ] ) [ ] [ ] ) \n",
      "\n",
      "\n",
      "BOS ( ) [ ] ( ) ( ) ( ) [ [ ] [ [ ] ) ( ) ) ( ( ) ) [ ( ( ) ) ] ] ] [ ( ( ) [ ( [ ] ) ] ( [ ] ) ( [ ] [ ] ) ( ) ( [ [ ] ] ) ( [ ] ) [ ( ) ( ) ] ( [ ( ) ] ( ) ) ) ] ( ( ( [ ] [ ] [ ] [ ] [ [ ] ( ) ] ( ( ) [ ] ) ) ( ( ( ) ) [ ] ( [ ] ) [ ( ) ] ) ( ) ) ( ) ( [ ( [ ] ) ] ( ) [ [ ( ) ( ) ] [ ] ( [ ] ( ) [ ] ( ) ) ] ) [ ] [ ] ) \n",
      "\n",
      "]\n",
      "BOS ( ) [ ] ( ) ( ) ( ) [ [ ] [ [  ) ( ) ) ( ( ) ) [ ( ( ) ) ] ] ] [ ( ( ) [ ( [ ] ) ] ( [ ] ) ( [ ] [ ] ) ( ) ( [ [ ] ] ) ( [ ] ) [ ( ) ( ) ] ( [ ( ) ] ( ) ) ) ] ( ( ( [ ] [ ] [ ] [ ] [ [ ] ( ) ] ( ( ) [ ] ) ) ( ( ( ) ) [ ] ( [ ] ) [ ( ) ] ) ( ) ) ( ) ( [ ( [ ] ) ] ( ) [ [ ( ) ( ) ] [ ] ( [ ] ( ) [ ] ( ) ) ] ) [ ] [ ] ) \n",
      "\n",
      "\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "correct = 0\n",
    "for source, gen in zip(sources[:len(each_sample_list)], each_sample_list):\n",
    "    sentence = re.sub(\"MASK\",gen,src)\n",
    "    sentence = re.sub(\"END\",\"\",sentence)\n",
    "    if is_dyck_2(sentence):\n",
    "        correct += 1\n",
    "    else:\n",
    "        print(gen)\n",
    "\n",
    "print(correct/len(each_sample_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(each_sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dyck_2(sentence):\n",
    "    \"\"\" Check if a sentence is dyck 2, with () and []\"\"\"\n",
    "    stack = []\n",
    "    for c in sentence:\n",
    "        if c == '(' or c == '[':\n",
    "            stack.append(c)\n",
    "        elif c == ')' or c == ']':\n",
    "            if len(stack) == 0: #or stack[-1] != '(':\n",
    "                return False\n",
    "            stack.pop()\n",
    "    return len(stack) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dyck_1(s):\n",
    "    stack = []\n",
    "    for c in s:\n",
    "        if c == '[':\n",
    "            stack.append(c)\n",
    "        elif c == ']':\n",
    "            if len(stack) == 0:\n",
    "                return False\n",
    "            else:\n",
    "                stack.pop()\n",
    "    if len(stack) == 0:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "valid_count = 0\n",
    "for data, gen in zip(test_data, each_sample_list):\n",
    "    dyck_string = re.sub(\"M\",gen,data[\"src\"])\n",
    "    if is_dyck_1(dyck_string):\n",
    "        valid_count += 1\n",
    "        print(\"src: \", data[\"src\"])\n",
    "        print(\"tgt: \", data[\"tgt\"])\n",
    "        print(\"gen: \", gen)\n",
    "        print(dyck_string)\n",
    "        print()\n",
    "    # else:\n",
    "    #     print(dyck_string)\n",
    "    #     print(gen)\n",
    "    #     print()\n",
    "print(\"Accuracy: \", valid_count/len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_plot(w, title=\"Attention plot\"):\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(w.detach().cpu().numpy(), cmap='viridis')\n",
    "\n",
    "    # Set ticks on the x-axis for every number\n",
    "    ax.set_xticks(range(w.shape[1]))\n",
    "    ax.set_xticklabels(range(0, w.shape[1])) \n",
    "    # make tick labels vertical\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "    # add legend\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    plt.title(title)\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot attentions\n",
    "w = model.transformer_blocks[0].attn2.attention_probs\n",
    "# w = w.mean(0)\n",
    "attention_plot(w.mean(0), title=f\"Attention plot al heads (mean)\")\n",
    "for i in range(w.shape[0]):\n",
    "        attention_plot(w[i], title=f\"Attention plot for head {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print token correspondence between tgt & gen\n",
    "for tgt, gen in zip([d[\"tgt\"].split(\" \") for d in data_piece], each_sample_list):\n",
    "    print(f\"----------\")\n",
    "    i = 0\n",
    "    for t, g in zip(tgt, gen.split(\" \")):\n",
    "        print(f\"{i}: {t} -> {g}\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
