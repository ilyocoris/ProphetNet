defaults:
  - dataset: recipes
  - tokenizer: bert
  - _self_

# Output Path
exp:
  seed: 101
  root: ./my_output
  name: eb6_d6_c768_lr8e-4_gc1.0
  dir: null

# Training Basics
batch_size: 50
device: cuda
lr_step: 100000
warmup_steps: 4000 # 4000 x grad_accum (from paper), thsi is done inside code
total_steps: 200000
lr: 8e-4
weight_decay: 0.0
grad_clip: 1.0 #-1.0
ema_rate: 0.9999
grad_accum: 8
eval_interval: 2000
log_interval: 500
save_interval: 20000

tgt_len: 128
max_pos_len: 512


# Model
model:
  mode: s2s
  pretrain: null

encoder:
  # this will overwrite hidden dim to 768
  initialize_from_pretrained: True
  layers: 6
  num_attention_heads: 8
  att_dropout: 0.1
  is_frozen: False # old fix_encoder

denoiser:
  layers: 6
  num_attention_heads: 8
  att_dropout: 0.1

time_channels: 128
in_channels: 768
out_channels: 768
diffusion_steps: 2000

vocab_size: 30522
intermediate_size: 3072
# num_attention_heads: 4
hidden_size: 768

# Diffusion
schedule_sampler: uniform  # uniform / xy_uniform / xy3_uniform / loss-second-moment


# *********** Random parameters  ***********

# src_lang: de
# tgt_lang: en

fairseq:
  use_fairseq: False
  real_data: False
  dist_data: False
use_mbert: False  # mt 
use_bpe: False  # mt 
pad_value: 0  # mt
num_workers: 4  # bpe->4

# training params


#*************************************
# something can change
clip_scale: 0.0
use_step_ratio: False
ratio_thre: 0.7
label_smooth: 0.0
scale_embedding: False

continue_train: False
# TODO: what is this? automatic mixed precision
use_AMP: True
grad_penalty: False
loss_aware: False
pred_len: False
length_factor: 0.1
init_weight: False
prediction: False
pred_len_strategy: null  # token_embed / mean_pool

att_strategy: txl #null  # txl / rotary / null
rel_postion: False
position_att: False
time_att: True
infer_self_condition: False
self_condition: False
end_point_scale: 2.0
dropout: 0.1
att_dropout: 0.1

num_samples: 1
ddim_sample: False
skip_timestep: 100

skip_sample: True
gen_timesteps: 20
#*************************************

# These parameters remain unchanged for now
predict_xstart: True
rescale_timesteps: True
resume_checkpoint: True

shared_embeds: False
roformer_timeAtt: False
add_layer_time: False
use_sentence_piece: False
load_encoder: False
predict_x_start: False
load_bart: False
use_kl: False
learn_pos: False

sigma_small: False
learn_sigma: False
rescale_learned_sigmas: False

logits_mode: 1  # 1 / 2
noise_schedule: sqrt
emb_type: random  # pretrain / random

# generate params
clip_denoised: True
load_from_ema: True
load_step: 0

# pretrain params
mask_pro: 0 #0.3
pre_max_len: 512

add_retrieval_sentences: False
retrieval_top_k: 1
