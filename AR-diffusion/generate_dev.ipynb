{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import hydra\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import load_states_from_checkpoint\n",
    "from data_utils.s2s_dataset import load_jsonl_data, S2S_dataset\n",
    "from model_utils.create_model import create_model, create_gaussian_diffusion\n",
    "from generate import denoised_fn_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake torch distributed\n",
    "from torch import distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "def initialize_distributed():\n",
    "    if not dist.is_initialized():\n",
    "        # Initialize the distributed environment\n",
    "        dist.init_process_group(backend='gloo')  # 'gloo' is suitable for local development\n",
    "\n",
    "# Call the initialization function\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1' \n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '1235'\n",
    "initialize_distributed()\n",
    "\n",
    "# Now you can use distributed functions safely\n",
    "rank = dist.get_rank()\n",
    "print(f\"Rank {rank} reporting in!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"reverse_words\"\n",
    "run = \"eb1_d1_c128_wd01\"\n",
    "# if hydra initialized, clear it\n",
    "if hydra.core.global_hydra.GlobalHydra.instance() is not None:\n",
    "    hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "hydra.initialize(config_path=f\"experiment_configs/{task}\")\n",
    "config = hydra.compose(config_name=f\"{run}.yaml\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 4\n",
    "print(config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.tokenizer.name_or_path)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "eval_model_path = f\"my_output/{task}/{run}/model/model_checkpoint-40000\"\n",
    "print(\"Load model from: \", eval_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and load it to device\n",
    "diffusion = create_gaussian_diffusion(config)\n",
    "model = create_model(config, vocab_size)\n",
    "model_saved_state = load_states_from_checkpoint(eval_model_path, dist.get_rank())\n",
    "model.load_state_dict(model_saved_state.model_dict)\n",
    "model.to(device)\n",
    "# sample text from random noise\n",
    "if config.ddim_sample:\n",
    "    sample_fn = (diffusion.ddim_sample_loop)\n",
    "else:\n",
    "    sample_fn = (diffusion.p_sample_loop)\n",
    "# word embedding\n",
    "emb_model = model.word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_piece = [\n",
    "    # {\n",
    "    #     \"src\": \"Mandarin Spinach Salad Ingredients:  [MASK]  Directions: -Mix 2 tablespoons sugar with small amount of water over low heat until sugar has melted. -Add almonds and stir until well coated. -Cook, then break apart. -Store at room temperature. -In large bowl, place lettuce, spinach, celery and onions. Immediately before serving, toss with Sweet-Sour Dressing. -Then add mandarin oranges and almonds. -Don't add Sweet-Sour Dressing until ready to serve; it will make the salad soggy.\",\n",
    "    #     \"tgt\":\"1/4 c. sliced almonds 1 head lettuce, torn into small pieces 1 bag spinach, torn into small pieces 2 medium stalks celery, chopped 2 green onions, thinly sliced 1 (11 oz.) can mandarin oranges, drained hard-boiled eggs (optional)\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\":\"Prune Bread Ingredients: 1 1/2 cups prunes dried, - without pits 2 cups all purpose flour 3/4 cup sugar 1 teaspoon baking soda 1/2 teaspoon salt 1 egg 2 tablespoons shortening melted Directions: -In medium saucepan, combine prunes with 1 1/2 cups water, bring to boiling. Reduce heat, and simmer, covered. 10 minutes. -Preheat oven 350 degrees. -[MASK] -Let cool in pan on wire rack 10 minutes. Remove from pan: cool completely on rack. Wrap in plastic film or foil, and refrigerate overnight before slicing.\",\n",
    "    #     \"tgt\":\"Sift flour with sugar, baking soda and salt; set aside. -Drain prunes, reserving liquid. Chop prunes or place in food processor and give a few good chops. -Add to reserved liquid, and measure. Add water if needed to make 2 cups. -In a large bowl combine egg and shortening: with electric mixer at medium speed, beat well. Add prune mixture: beat until well blended. -Add flour mixture: beat at low speed just until smooth. Turn into prepared pan. -Bake 50-60 minutes, or until cake tester inserted in center comes out clean.\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\":\"No Bake Chocolate Cookies Ingredients: 1/4 lb. margarine 1/2 c. milk 1/2 c. chocolate bits 2 c. sugar 3 c. oatmeal 1/2 c. peanut butter 1 tsp. vanilla Directions: -[MASK] -Remove from heat and add oatmeal, peanut butter and vanilla. -Cool slightly and drop by teaspoons on waxed paper.\",\n",
    "    #     \"tgt\": \"Bring butter, milk, chocolate and sugar to a boil in a large saucepan. Boil and stir continuously for 1 minute.\"\n",
    "    # }\n",
    "    # {\n",
    "    #     \"src\": \"non mcdowell explosion\",\n",
    "    #     \"tgt\": \"explosion mcdowell non\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\": \"angels aldo ##oulos slovenian\",\n",
    "    #     \"tgt\": \"slovenian ##oulos aldo angels\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\": \"eased mention ##bine outcomes ##efe aldo ##糹 jumped ##claim weakness\",\n",
    "    #     \"tgt\": \"weakness ##claim jumped ##糹 ##efe outcomes ##bine mention ##aldo eased\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\":\"332 irritation halftime transitional escaping explosion wehrmacht mention flavors eased agrarian charlton appropriately europa dissemination publicized achieved intensity churchyard slaves ェ\",\n",
    "    #     \"tgt\":\"ェ slaves churchyard intensity achieved publicized dissemination europa appropriately charlton agrarian eased flavors mention wehrmacht explosion escaping transitional halftime irritation 332\"\n",
    "    # },\n",
    "#     {\n",
    "#         \"src\":\"irritation halftime transitional escaping explosion wehrmacht elector mention flavors eased agrarian charlton appropriately europa dissemination publicized achieved intensity churchyard slaves ェ\",\n",
    "#         \"tgt\":\"ェ slaves churchyard intensity achieved publicized dissemination europa appropriately charlton agrarian eased flavors mention elector wehrmacht explosion escaping transitional halftime irritation\"\n",
    "#     },\n",
    "# {\n",
    "#         \"src\":\"irritation halftime transitional escaping explosion wehrmacht bee mention flavors eased agrarian charlton appropriately europa dissemination publicized achieved intensity churchyard slaves ェ\",\n",
    "#         \"tgt\":\"ェ slaves churchyard intensity achieved publicized dissemination europa appropriately charlton agrarian eased flavors mention bee wehrmacht explosion escaping transitional halftime irritation\"\n",
    "    # },\n",
    "    {\n",
    "        \"src\":\"electors deliberate irritation halftime transitional escaping explosion wehrmacht mention flavors eased agrarian charlton appropriately europa dissemination publicized achieved intensity churchyard slaves ェ\",\n",
    "        \"tgt\":\"ェ slaves churchyard intensity achieved publicized dissemination europa appropriately charlton agrarian eased flavors mention wehrmacht explosion escaping transitional halftime irritation deliberate electors\"\n",
    "    },\n",
    "    # {\n",
    "    #     \"src\": \"transitional\",\n",
    "    #     \"tgt\": \"transitional\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\": \"37 63 89 28 43 57 10 33\",\n",
    "    #     \"tgt\": \"37 63 89 28 43 57 10 33\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\":\"34\",\n",
    "    #     \"tgt\":\"34\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\":\"44 31\",\n",
    "    #     \"tgt\":\"44 31\"\n",
    "    # },\n",
    "\n",
    "    # {\n",
    "    #     \"src\":\"517 183 293 17 604 595 422 411 426 659 247 279 339 978 324 44 322 67 402 28 43 57 10 33\",\n",
    "    #     \"tgt\":\"517 183 293 17 604 595 422 411 426 659 247 279 339 978 324 44 322 67 402 28 43 57 10 33\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\":\"67 99 56\",\n",
    "    #     \"tgt\":\"67 99 56\"\n",
    "    # },\n",
    "    \n",
    "]\n",
    "\n",
    "dev_dataset = S2S_dataset(data_piece, tokenizer, config)\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_dataset, batch_size=config.batch_size, \n",
    "    drop_last=False, pin_memory=True, num_workers=config.num_workers, \n",
    "    collate_fn=S2S_dataset.get_collate_fn(config)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 1 sample for each data\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "each_sample_list = []\n",
    "\n",
    "for _, batch in enumerate(tqdm(dev_dataloader)):\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden_states = model.encoder(\n",
    "            input_ids=batch['src_input_ids'].to(device), \n",
    "            attention_mask=batch['src_attention_mask'].to(device),\n",
    "        ).last_hidden_state  # [bs, seq_len, hz]\n",
    "\n",
    "    if config.pred_len:\n",
    "        with torch.no_grad():\n",
    "            length_out = model.get_pred_len(\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                src_masks=batch['src_attention_mask'].to(device),\n",
    "                normalize=True,\n",
    "            )  # [bs, max_pos_len]\n",
    "            pred_lengs = length_out.max(-1)[1]  # [bs,], max return tuple(value, indices)\n",
    "\n",
    "        tgt_attention_mask = []\n",
    "        for len_item in pred_lengs:\n",
    "            tgt_attention_mask.append([1] * len_item + [0] * (max(pred_lengs) - len_item))\n",
    "        tgt_attention_mask = torch.tensor(tgt_attention_mask).long()\n",
    "        \n",
    "        input_shape = (\n",
    "            tgt_attention_mask.shape[0], tgt_attention_mask.shape[1], config.in_channels,\n",
    "        )\n",
    "    else:\n",
    "        pred_lengs, tgt_attention_mask = None, None\n",
    "        input_shape = (\n",
    "            batch['src_input_ids'].shape[0], config.tgt_len, config.in_channels,\n",
    "        )\n",
    "\n",
    "    model_kwargs = {'src_attention_mask': batch['src_attention_mask'].to(device),\n",
    "                    'tgt_attention_mask': tgt_attention_mask,\n",
    "                    'encoder_hidden_states': encoder_hidden_states,}\n",
    "\n",
    "    sample = sample_fn(\n",
    "        model,\n",
    "        input_shape,\n",
    "        clip_denoised=config.clip_denoised,\n",
    "        # \"Freeze\" some parameters for easy recall.\n",
    "        denoised_fn=partial(denoised_fn_round,\n",
    "                            config, emb_model.to(device)),\n",
    "        progress=True,\n",
    "        model_kwargs=model_kwargs,\n",
    "        pred_lengs=pred_lengs,\n",
    "        top_p=-1.0,\n",
    "    )\n",
    "\n",
    "\n",
    "    logits = model.get_logits(sample)  # (bs, seq_len, vocab_size)\n",
    "    sample_id_tensor = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    \n",
    "    each_sample_list.extend(tokenizer.batch_decode(sample_id_tensor, skip_special_tokens=True))\n",
    "\n",
    "    print(tokenizer.batch_decode(sample_id_tensor, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_plot(w, title=\"Attention plot\"):\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(w.detach().cpu().numpy(), cmap='viridis')\n",
    "\n",
    "    # Set ticks on the x-axis for every number\n",
    "    ax.set_xticks(range(w.shape[1]))\n",
    "    ax.set_xticklabels(range(0, w.shape[1])) \n",
    "    # make tick labels vertical\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "    # add legend\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    plt.title(title)\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot attentions\n",
    "w = model.transformer_blocks[0].attn1.attention_probs\n",
    "# w = w.mean(0)\n",
    "attention_plot(w.mean(0), title=f\"Attention plot al heads (mean)\")\n",
    "for i in range(w.shape[0]):\n",
    "        attention_plot(w[i], title=f\"Attention plot for head {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print token correspondence between tgt & gen\n",
    "for tgt, gen in zip([d[\"tgt\"].split(\" \") for d in data_piece], each_sample_list):\n",
    "    print(f\"----------\")\n",
    "    i = 0\n",
    "    for t, g in zip(tgt, gen.split(\" \")):\n",
    "        print(f\"{i}: {t} -> {g}\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
