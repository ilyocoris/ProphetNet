{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gcilleru/ProphetNet/AR-diffusion/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import hydra\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import load_states_from_checkpoint\n",
    "from data_utils.s2s_dataset import load_jsonl_data, S2S_dataset\n",
    "from model_utils.create_model import create_model, create_gaussian_diffusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0 reporting in!\n"
     ]
    }
   ],
   "source": [
    "# fake torch distributed\n",
    "from torch import distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "def initialize_distributed():\n",
    "    if not dist.is_initialized():\n",
    "        # Initialize the distributed environment\n",
    "        dist.init_process_group(backend='gloo')  # 'gloo' is suitable for local development\n",
    "\n",
    "# Call the initialization function\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1' \n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '1235'\n",
    "initialize_distributed()\n",
    "\n",
    "# Now you can use distributed functions safely\n",
    "rank = dist.get_rank()\n",
    "print(f\"Rank {rank} reporting in!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2788262/930210726.py:1: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  hydra.initialize(config_path=\".\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydra.initialize(config_path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vanilla_CrossAttention_Diffusion_LM(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(10000, 256, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 256)\n",
       "      (token_type_embeddings): Embedding(2, 256)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (input_up_proj): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=512, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       "  (output_down_proj): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "  )\n",
       "  (word_embedding): Embedding(10000, 64)\n",
       "  (position_embeddings): Embedding(512, 256)\n",
       "  (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=64, out_features=10000, bias=True)\n",
       "  (time_trans): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0): BasicTransformerBlock(\n",
       "      (attn1): CrossAttention(\n",
       "        (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (to_k): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (to_v): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (dropatt): Dropout(p=0.1, inplace=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (attn2): CrossAttention(\n",
       "        (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (to_k): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (to_v): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (dropatt): Dropout(p=0.1, inplace=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vanilla model\n",
    "config = hydra.compose(config_name=\"config_vanilla.yaml\")\n",
    "model = create_model(config, config.vocab_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = hydra.compose(config_name=\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA_VISIBLE_DEVICES=6 torchrun --nproc_per_node=1 --nnodes=1 --master_port 29501 generate.py \\\n",
    "# model.name='bert-base-uncased' batch_size=128 \\\n",
    "# exp.name=$FILE_NAME load_step=$STEP \\\n",
    "# data.name=$DATA_NAME data.path=$DATA_PATH tgt_len=32 max_pos_len=32 num_samples=50 \\\n",
    "# intermediate_size=2048 num_attention_heads=8 dropout=0.2 \\\n",
    "# in_channels=128 out_channels=128 time_channels=128 \\\n",
    "# skip_sample=False gen_timesteps=2000 \\\n",
    "# schedule_sampler='uniform' time_att=True att_strategy='txl' load_from_ema=False prediction=True \\\n",
    "# fix_encoder=True model.custom_denoiser=True model.denoiser_layers=1 \\\n",
    "\n",
    "\n",
    "config\n",
    "\n",
    "config.exp.name=\"d1_uni\"\n",
    "config.load_step=20000\n",
    "config.data.name=\"reverse\"\n",
    "config.data.path=\"data/raw/sequence\"\n",
    "\n",
    "config.model.name = 'bert-base-uncased'\n",
    "config.tgt_len=24\n",
    "config.max_pos_len=24\n",
    "config.num_samples=1 # how many generations for each sample\n",
    "\n",
    "config.intermediate_size=2048\n",
    "config.num_attention_heads=8\n",
    "config.dropout=0.2\n",
    "config.in_channels=128\n",
    "config.out_channels=128\n",
    "config.time_channels=128\n",
    "\n",
    "config.skip_sample=False\n",
    "config.gen_timesteps=2000\n",
    "config.schedule_sampler='uniform'\n",
    "config.time_att=True\n",
    "config.att_strategy='txl'\n",
    "\n",
    "config.load_from_ema=False\n",
    "config.prediction=True\n",
    "\n",
    "# config.fix_encoder=True\n",
    "config.model.custom_denoiser=True\n",
    "config.model.denoiser_layers=1\n",
    "\n",
    "config.batch_size = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate path:  ./my_output/reverse/d1_uni/20000_un_100\n",
      "Exp dir:  ./my_output/reverse/d1_uni\n"
     ]
    }
   ],
   "source": [
    "config.exp.dir = os.path.join(config.exp.root, config.data.name, config.exp.name)\n",
    "generate_path = os.path.join(config.exp.dir, str(config.load_step))\n",
    "if config.load_from_ema:\n",
    "    generate_path += ('_ema_' + str(config.ema_rate))\n",
    "if config.clip_denoised:\n",
    "    generate_path += '_clip_denoised_'\n",
    "if config.infer_self_condition:\n",
    "    generate_path += '_selfcond_'\n",
    "if config.skip_sample:\n",
    "    generate_path += '_skip_'\n",
    "if config.ddim_sample:\n",
    "    generate_path += '_ddim_'\n",
    "\n",
    "if config.schedule_sampler == 'xy_uniform':\n",
    "    generate_path += ('_xy_' + str(config.gen_timesteps))\n",
    "else:\n",
    "    generate_path += ('_un_' + str(config.skip_timestep))\n",
    "\n",
    "print(\"Generate path: \", generate_path)\n",
    "print(\"Exp dir: \", config.exp.dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model.name)\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model from:  my_output/reverse_words/revseqw_l1_te/model/model_checkpoint-50000\n"
     ]
    }
   ],
   "source": [
    "# if config.load_from_ema:\n",
    "#     eval_model_path = os.path.join(\n",
    "#         config.exp.dir, 'model', f'ema_{config.ema_rate}_checkpoint-{config.load_step}')\n",
    "# else:\n",
    "#     eval_model_path = os.path.join(\n",
    "#         config.exp.dir, 'model', f'model_checkpoint-{config.load_step}')\n",
    "# eval_model_path = \"data/models/GENIE_ckpt-500w\"\n",
    "# eval_model_path = \"my_output/reverse/d1_uni/model/model_checkpoint-80000\"\n",
    "eval_model_path = \"my_output/reverse_words/revseqw_l1_te/model/model_checkpoint-50000\"\n",
    "print(\"Load model from: \", eval_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion = create_gaussian_diffusion(config)\n",
    "model = create_model(config, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_saved_state = load_states_from_checkpoint(eval_model_path, dist.get_rank())\n",
    "model.load_state_dict(model_saved_state.model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossAttention_Diffusion_LM(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
       "      (position_embeddings): Embedding(24, 512)\n",
       "      (token_type_embeddings): Embedding(2, 512)\n",
       "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (input_up_proj): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  )\n",
       "  (output_down_proj): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  )\n",
       "  (word_embedding): Embedding(30522, 128)\n",
       "  (position_embeddings): Embedding(24, 512)\n",
       "  (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (lm_head): Linear(in_features=128, out_features=30522, bias=True)\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0): BasicTransformerBlock(\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (time_trans): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (attn1): CrossAttention(\n",
       "        (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (dropatt): Dropout(p=0.1, inplace=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=512, out_features=4096, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.2, inplace=False)\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (attn2): CrossAttention(\n",
       "        (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (dropatt): Dropout(p=0.1, inplace=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample text from random noise\n",
    "if config.ddim_sample:\n",
    "    sample_fn = (diffusion.ddim_sample_loop)\n",
    "else:\n",
    "    sample_fn = (diffusion.p_sample_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = model.word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_piece = [\n",
    "    {\n",
    "        \"src\": \"non mcdowell explosion\",\n",
    "        \"tgt\": \"explosion mcdowell non\",\n",
    "    },\n",
    "    {\n",
    "        \"src\": \"angels aldo ##oulos slovenian\",\n",
    "        \"tgt\": \"slovenian ##oulos aldo angels\",\n",
    "    },\n",
    "    {\n",
    "        \"src\": \"eased mention ##bine outcomes ##efe aldo ##糹 jumped ##claim weakness\",\n",
    "        \"tgt\": \"weakness ##claim jumped ##糹 ##efe outcomes ##bine mention ##aldo eased\",\n",
    "    },\n",
    "    # {\n",
    "    #     \"src\": \"37 63 89 28 43 57 10 33\",\n",
    "    #     \"tgt\": \"33 10 57 43 28 89 63 37\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\":\"34\",\n",
    "    #     \"tgt\":\"34\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\":\"44 31\",\n",
    "    #     \"tgt\":\"31 44\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\":\"67 99 56\",\n",
    "    #     \"tgt\":\"56 99 67\"\n",
    "    # }\n",
    "]\n",
    "\n",
    "dev_dataset = S2S_dataset(data_piece, tokenizer, config)\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_dataset, batch_size=config.batch_size, \n",
    "    drop_last=False, pin_memory=True, num_workers=config.num_workers, \n",
    "    collate_fn=S2S_dataset.get_collate_fn(config)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "def denoised_fn_round(config, emb_model, text_emb, t):\n",
    "    down_proj_emb = emb_model.weight  # (vocab_size, embed_dim)\n",
    "\n",
    "    old_shape = text_emb.shape\n",
    "    old_device = text_emb.device\n",
    "\n",
    "    def get_efficient_knn(down_proj_emb, text_emb, dist='l2'):\n",
    "        if dist == 'l2':\n",
    "            emb_norm = (down_proj_emb ** 2).sum(-1).view(-1, 1)  # (vocab, 1)\n",
    "            text_emb_t = torch.transpose(text_emb.view(-1, text_emb.size(-1)), 0, 1)  # (emb_dim, bs*seqlen)\n",
    "            arr_norm = (text_emb ** 2).sum(-1).view(-1, 1)  # (bs*seqlen, 1)\n",
    "            # down_proj_emb: (vocab, emb_dim), text_emb_t:(emb_dim, bs*seqlen)\n",
    "            # a+b automatically broadcasts to the same dimension i.e. (vocab, bs*seqlen)\n",
    "            dist = emb_norm + arr_norm.transpose(0, 1) - 2.0 * torch.mm(down_proj_emb, text_emb_t) \n",
    "            dist = torch.clamp(dist, 0.0, np.inf)  # Limit the value of input to [min, max].\n",
    "        # Select the smallest distance in the vocab dimension, \n",
    "        # that is, select bs*seq_len most likely words from all vocabs.\n",
    "        topk_out = torch.topk(-dist, k=1, dim=0)\n",
    "\n",
    "        return topk_out.values, topk_out.indices  # logits, token_id (1, bs*seq_len)\n",
    "\n",
    "    dist = 'l2'\n",
    "    if len(text_emb.shape) > 2:\n",
    "        text_emb = text_emb.reshape(-1, text_emb.size(-1))\n",
    "    else:\n",
    "        text_emb = text_emb\n",
    "\n",
    "    val, indices = get_efficient_knn(down_proj_emb,\n",
    "                                     text_emb.to(down_proj_emb.device), dist=dist)\n",
    "    rounded_tokens = indices[0]  # (bs*seq_len,)\n",
    "    new_embeds = emb_model(rounded_tokens).view(old_shape).to(old_device)\n",
    "\n",
    "    return new_embeds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 1 sample for each data\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "each_sample_list = []\n",
    "\n",
    "for _, batch in enumerate(tqdm(dev_dataloader)):\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden_states = model.encoder(\n",
    "            input_ids=batch['src_input_ids'].to(device), \n",
    "            attention_mask=batch['src_attention_mask'].to(device),\n",
    "        ).last_hidden_state  # [bs, seq_len, hz]\n",
    "\n",
    "    if config.pred_len:\n",
    "        with torch.no_grad():\n",
    "            length_out = model.get_pred_len(\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                src_masks=batch['src_attention_mask'].to(device),\n",
    "                normalize=True,\n",
    "            )  # [bs, max_pos_len]\n",
    "            pred_lengs = length_out.max(-1)[1]  # [bs,], max return tuple(value, indices)\n",
    "\n",
    "        tgt_attention_mask = []\n",
    "        for len_item in pred_lengs:\n",
    "            tgt_attention_mask.append([1] * len_item + [0] * (max(pred_lengs) - len_item))\n",
    "        tgt_attention_mask = torch.tensor(tgt_attention_mask).long()\n",
    "        \n",
    "        input_shape = (\n",
    "            tgt_attention_mask.shape[0], tgt_attention_mask.shape[1], config.in_channels,\n",
    "        )\n",
    "    else:\n",
    "        pred_lengs, tgt_attention_mask = None, None\n",
    "        input_shape = (\n",
    "            batch['src_input_ids'].shape[0], config.tgt_len, config.in_channels,\n",
    "        )\n",
    "\n",
    "    model_kwargs = {'src_attention_mask': batch['src_attention_mask'].to(device),\n",
    "                    'tgt_attention_mask': tgt_attention_mask,\n",
    "                    'encoder_hidden_states': encoder_hidden_states,}\n",
    "\n",
    "    sample = sample_fn(\n",
    "        model,\n",
    "        input_shape,\n",
    "        clip_denoised=config.clip_denoised,\n",
    "        # \"Freeze\" some parameters for easy recall.\n",
    "        denoised_fn=partial(denoised_fn_round,\n",
    "                            config, emb_model.to(device)),\n",
    "        progress=True,\n",
    "        model_kwargs=model_kwargs,\n",
    "        pred_lengs=pred_lengs,\n",
    "        top_p=-1.0,\n",
    "    )\n",
    "\n",
    "\n",
    "    logits = model.get_logits(sample)  # (bs, seq_len, vocab_size)\n",
    "    sample_id_tensor = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    \n",
    "    each_sample_list.extend(tokenizer.batch_decode(sample_id_tensor, skip_special_tokens=True))\n",
    "\n",
    "    print(tokenizer.batch_decode(sample_id_tensor, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update changes on gaussian_diffusion.py with autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for sample in diffusion.p_sample_loop_progressive(\n",
    "                    model,\n",
    "                    input_shape,\n",
    "                    noise=None,\n",
    "                    clip_denoised=config.clip_denoised,\n",
    "                    denoised_fn=partial(denoised_fn_round,\n",
    "                            config, emb_model.to(device)),\n",
    "                    model_kwargs=model_kwargs,\n",
    "                    device=device,\n",
    "                    progress=True,\n",
    "                    top_p=-1.0,\n",
    "                ):\n",
    "                    final = sample\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer_blocks[0].attn1.cx_attention_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model.transformer_blocks[0].attn1.cx_attention_probs\n",
    "# average over the 8 attention heads\n",
    "w = w.mean(0)\n",
    "# print heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(w.detach().cpu().numpy())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Src:\", [d[\"src\"] for d in data_piece])\n",
    "print(\"Tgt:\", [d[\"tgt\"] for d in data_piece])\n",
    "print(\"Sample:\", each_sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
