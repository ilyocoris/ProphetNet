{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import hydra\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import load_states_from_checkpoint\n",
    "from data_utils.s2s_dataset import load_jsonl_data, S2S_dataset\n",
    "from model_utils.create_model import create_model, create_gaussian_diffusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake torch distributed\n",
    "from torch import distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "def initialize_distributed():\n",
    "    if not dist.is_initialized():\n",
    "        # Initialize the distributed environment\n",
    "        dist.init_process_group(backend='gloo')  # 'gloo' is suitable for local development\n",
    "\n",
    "# Call the initialization function\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1' \n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '1235'\n",
    "initialize_distributed()\n",
    "\n",
    "# Now you can use distributed functions safely\n",
    "rank = dist.get_rank()\n",
    "print(f\"Rank {rank} reporting in!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra.initialize(config_path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanilla model\n",
    "config = hydra.compose(config_name=\"config_recipes.yaml\")\n",
    "model = create_model(config, config.vocab_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = hydra.compose(config_name=\"config_recipes.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA_VISIBLE_DEVICES=6 torchrun --nproc_per_node=1 --nnodes=1 --master_port 29501 generate.py \\\n",
    "# model.name='bert-base-uncased' batch_size=128 \\\n",
    "# exp.name=$FILE_NAME load_step=$STEP \\\n",
    "# data.name=$DATA_NAME data.path=$DATA_PATH tgt_len=32 max_pos_len=32 num_samples=50 \\\n",
    "# intermediate_size=2048 num_attention_heads=8 dropout=0.2 \\\n",
    "# in_channels=128 out_channels=128 time_channels=128 \\\n",
    "# skip_sample=False gen_timesteps=2000 \\\n",
    "# schedule_sampler='uniform' time_att=True att_strategy='txl' load_from_ema=False prediction=True \\\n",
    "# fix_encoder=True model.custom_denoiser=True model.denoiser_layers=1 \\\n",
    "\n",
    "\n",
    "config\n",
    "\n",
    "config.exp.name=\"d1_uni\"\n",
    "config.load_step=20000\n",
    "config.data.name=\"reverse\"\n",
    "config.data.path=\"data/raw/sequence\"\n",
    "\n",
    "config.model.name = 'bert-base-uncased'\n",
    "config.tgt_len=128\n",
    "config.max_pos_len=512\n",
    "config.num_samples=1 # how many generations for each sample\n",
    "\n",
    "config.intermediate_size=2048\n",
    "config.num_attention_heads=8\n",
    "config.dropout=0.2\n",
    "config.in_channels=128\n",
    "config.out_channels=128\n",
    "config.time_channels=128\n",
    "\n",
    "config.skip_sample=False\n",
    "config.gen_timesteps=2000\n",
    "config.schedule_sampler='uniform'\n",
    "config.time_att=True\n",
    "config.att_strategy='txl'\n",
    "\n",
    "config.load_from_ema=False\n",
    "config.prediction=True\n",
    "\n",
    "# config.fix_encoder=True\n",
    "config.model.custom_denoiser=False\n",
    "config.model.denoiser_layers=1\n",
    "\n",
    "config.batch_size = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate path:  ./my_output/identity/eb1_d1_c128_wd01/0_un_100\n",
      "Exp dir:  ./my_output/identity/eb1_d1_c128_wd01\n"
     ]
    }
   ],
   "source": [
    "config.exp.dir = os.path.join(config.exp.root, config.data.name, config.exp.name)\n",
    "generate_path = os.path.join(config.exp.dir, str(config.load_step))\n",
    "if config.load_from_ema:\n",
    "    generate_path += ('_ema_' + str(config.ema_rate))\n",
    "if config.clip_denoised:\n",
    "    generate_path += '_clip_denoised_'\n",
    "if config.infer_self_condition:\n",
    "    generate_path += '_selfcond_'\n",
    "if config.skip_sample:\n",
    "    generate_path += '_skip_'\n",
    "if config.ddim_sample:\n",
    "    generate_path += '_ddim_'\n",
    "\n",
    "if config.schedule_sampler == 'xy_uniform':\n",
    "    generate_path += ('_xy_' + str(config.gen_timesteps))\n",
    "else:\n",
    "    generate_path += ('_un_' + str(config.skip_timestep))\n",
    "\n",
    "print(\"Generate path: \", generate_path)\n",
    "print(\"Exp dir: \", config.exp.dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.tokenizer.name_or_path)\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model from:  my_output/identity/eb1_d1_c128_wd01/model/model_checkpoint-40000\n"
     ]
    }
   ],
   "source": [
    "# if config.load_from_ema:\n",
    "#     eval_model_path = os.path.join(\n",
    "#         config.exp.dir, 'model', f'ema_{config.ema_rate}_checkpoint-{config.load_step}')\n",
    "# else:\n",
    "#     eval_model_path = os.path.join(\n",
    "#         config.exp.dir, 'model', f'model_checkpoint-{config.load_step}')\n",
    "# eval_model_path = \"data/models/GENIE_ckpt-500w\"\n",
    "# eval_model_path = \"my_output/reverse/d1_uni/model/model_checkpoint-80000\"\n",
    "eval_model_path = \"my_output/identity/eb1_d1_c128_wd01/model/model_checkpoint-40000\"\n",
    "# eval_model_path = \"my_output/recipes/ebert_d6_c128/model/model_checkpoint-30000\"\n",
    "print(\"Load model from: \", eval_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion = create_gaussian_diffusion(config)\n",
    "model = create_model(config, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_saved_state = load_states_from_checkpoint(eval_model_path, dist.get_rank())\n",
    "model.load_state_dict(model_saved_state.model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossAttention_Diffusion_LM(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (input_up_proj): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=1536, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  )\n",
       "  (output_down_proj): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1536, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=1536, out_features=128, bias=True)\n",
       "  )\n",
       "  (word_embedding): Embedding(30522, 128)\n",
       "  (position_embeddings): Embedding(24, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=128, out_features=30522, bias=True)\n",
       "  (time_trans): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0): BasicTransformerBlock(\n",
       "      (attn1): CrossAttention(\n",
       "        (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (to_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (to_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropatt): Dropout(p=0.1, inplace=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=768, out_features=6144, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (attn2): CrossAttention(\n",
       "        (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (to_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (to_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropatt): Dropout(p=0.1, inplace=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample text from random noise\n",
    "if config.ddim_sample:\n",
    "    sample_fn = (diffusion.ddim_sample_loop)\n",
    "else:\n",
    "    sample_fn = (diffusion.p_sample_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = model.word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_piece = [\n",
    "    # {\n",
    "    #     \"src\": \"Mandarin Spinach Salad Ingredients:  [MASK]  Directions: -Mix 2 tablespoons sugar with small amount of water over low heat until sugar has melted. -Add almonds and stir until well coated. -Cook, then break apart. -Store at room temperature. -In large bowl, place lettuce, spinach, celery and onions. Immediately before serving, toss with Sweet-Sour Dressing. -Then add mandarin oranges and almonds. -Don't add Sweet-Sour Dressing until ready to serve; it will make the salad soggy.\",\n",
    "    #     \"tgt\":\"1/4 c. sliced almonds 1 head lettuce, torn into small pieces 1 bag spinach, torn into small pieces 2 medium stalks celery, chopped 2 green onions, thinly sliced 1 (11 oz.) can mandarin oranges, drained hard-boiled eggs (optional)\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\":\"Prune Bread Ingredients: 1 1/2 cups prunes dried, - without pits 2 cups all purpose flour 3/4 cup sugar 1 teaspoon baking soda 1/2 teaspoon salt 1 egg 2 tablespoons shortening melted Directions: -In medium saucepan, combine prunes with 1 1/2 cups water, bring to boiling. Reduce heat, and simmer, covered. 10 minutes. -Preheat oven 350 degrees. -[MASK] -Let cool in pan on wire rack 10 minutes. Remove from pan: cool completely on rack. Wrap in plastic film or foil, and refrigerate overnight before slicing.\",\n",
    "    #     \"tgt\":\"Sift flour with sugar, baking soda and salt; set aside. -Drain prunes, reserving liquid. Chop prunes or place in food processor and give a few good chops. -Add to reserved liquid, and measure. Add water if needed to make 2 cups. -In a large bowl combine egg and shortening: with electric mixer at medium speed, beat well. Add prune mixture: beat until well blended. -Add flour mixture: beat at low speed just until smooth. Turn into prepared pan. -Bake 50-60 minutes, or until cake tester inserted in center comes out clean.\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\":\"No Bake Chocolate Cookies Ingredients: 1/4 lb. margarine 1/2 c. milk 1/2 c. chocolate bits 2 c. sugar 3 c. oatmeal 1/2 c. peanut butter 1 tsp. vanilla Directions: -[MASK] -Remove from heat and add oatmeal, peanut butter and vanilla. -Cool slightly and drop by teaspoons on waxed paper.\",\n",
    "    #     \"tgt\": \"Bring butter, milk, chocolate and sugar to a boil in a large saucepan. Boil and stir continuously for 1 minute.\"\n",
    "    # }\n",
    "    # {\n",
    "    #     \"src\": \"non mcdowell explosion\",\n",
    "    #     \"tgt\": \"explosion mcdowell non\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\": \"angels aldo ##oulos slovenian\",\n",
    "    #     \"tgt\": \"slovenian ##oulos aldo angels\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\": \"eased mention ##bine outcomes ##efe aldo ##糹 jumped ##claim weakness\",\n",
    "    #     \"tgt\": \"weakness ##claim jumped ##糹 ##efe outcomes ##bine mention ##aldo eased\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\":\"332 irritation halftime transitional escaping explosion wehrmacht mention flavors eased agrarian charlton appropriately europa dissemination publicized achieved intensity churchyard slaves ェ\",\n",
    "    #     \"tgt\":\"ェ slaves churchyard intensity achieved publicized dissemination europa appropriately charlton agrarian eased flavors mention wehrmacht explosion escaping transitional halftime irritation 332\"\n",
    "    # },\n",
    "#     {\n",
    "#         \"src\":\"irritation halftime transitional escaping explosion wehrmacht elector mention flavors eased agrarian charlton appropriately europa dissemination publicized achieved intensity churchyard slaves ェ\",\n",
    "#         \"tgt\":\"ェ slaves churchyard intensity achieved publicized dissemination europa appropriately charlton agrarian eased flavors mention elector wehrmacht explosion escaping transitional halftime irritation\"\n",
    "#     },\n",
    "# {\n",
    "#         \"src\":\"irritation halftime transitional escaping explosion wehrmacht bee mention flavors eased agrarian charlton appropriately europa dissemination publicized achieved intensity churchyard slaves ェ\",\n",
    "#         \"tgt\":\"ェ slaves churchyard intensity achieved publicized dissemination europa appropriately charlton agrarian eased flavors mention bee wehrmacht explosion escaping transitional halftime irritation\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\":\"electors deliberate irritation halftime transitional escaping explosion wehrmacht mention flavors eased agrarian charlton appropriately europa dissemination publicized achieved intensity churchyard slaves ェ\",\n",
    "    #     \"tgt\":\"ェ slaves churchyard intensity achieved publicized dissemination europa appropriately charlton agrarian eased flavors mention wehrmacht explosion escaping transitional halftime irritation deliberate electors\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\": \"transitional\",\n",
    "    #     \"tgt\": \"transitional\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\": \"37 63 89 28 43 57 10 33\",\n",
    "    #     \"tgt\": \"37 63 89 28 43 57 10 33\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\":\"34\",\n",
    "    #     \"tgt\":\"34\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\":\"44 31\",\n",
    "    #     \"tgt\":\"44 31\"\n",
    "    # },\n",
    "\n",
    "    {\n",
    "        \"src\":\"517 183 293 17 604 595 422 411 426 659 247 279 339 978 324 44 322 67 402 28 43 57 10 33\",\n",
    "        \"tgt\":\"517 183 293 17 604 595 422 411 426 659 247 279 339 978 324 44 322 67 402 28 43 57 10 33\"\n",
    "    },\n",
    "    {\n",
    "        \"src\":\"67 99 56\",\n",
    "        \"tgt\":\"67 99 56\"\n",
    "    },\n",
    "    \n",
    "]\n",
    "\n",
    "dev_dataset = S2S_dataset(data_piece, tokenizer, config)\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_dataset, batch_size=config.batch_size, \n",
    "    drop_last=False, pin_memory=True, num_workers=config.num_workers, \n",
    "    collate_fn=S2S_dataset.get_collate_fn(config)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "def denoised_fn_round(config, emb_model, text_emb, t):\n",
    "    down_proj_emb = emb_model.weight  # (vocab_size, embed_dim)\n",
    "\n",
    "    old_shape = text_emb.shape\n",
    "    old_device = text_emb.device\n",
    "\n",
    "    def get_efficient_knn(down_proj_emb, text_emb, dist='l2'):\n",
    "        if dist == 'l2':\n",
    "            emb_norm = (down_proj_emb ** 2).sum(-1).view(-1, 1)  # (vocab, 1)\n",
    "            text_emb_t = torch.transpose(text_emb.view(-1, text_emb.size(-1)), 0, 1)  # (emb_dim, bs*seqlen)\n",
    "            arr_norm = (text_emb ** 2).sum(-1).view(-1, 1)  # (bs*seqlen, 1)\n",
    "            # down_proj_emb: (vocab, emb_dim), text_emb_t:(emb_dim, bs*seqlen)\n",
    "            # a+b automatically broadcasts to the same dimension i.e. (vocab, bs*seqlen)\n",
    "            dist = emb_norm + arr_norm.transpose(0, 1) - 2.0 * torch.mm(down_proj_emb, text_emb_t) \n",
    "            dist = torch.clamp(dist, 0.0, np.inf)  # Limit the value of input to [min, max].\n",
    "        # Select the smallest distance in the vocab dimension, \n",
    "        # that is, select bs*seq_len most likely words from all vocabs.\n",
    "        topk_out = torch.topk(-dist, k=1, dim=0)\n",
    "\n",
    "        return topk_out.values, topk_out.indices  # logits, token_id (1, bs*seq_len)\n",
    "\n",
    "    dist = 'l2'\n",
    "    if len(text_emb.shape) > 2:\n",
    "        text_emb = text_emb.reshape(-1, text_emb.size(-1))\n",
    "    else:\n",
    "        text_emb = text_emb\n",
    "\n",
    "    val, indices = get_efficient_knn(down_proj_emb,\n",
    "                                     text_emb.to(down_proj_emb.device), dist=dist)\n",
    "    rounded_tokens = indices[0]  # (bs*seq_len,)\n",
    "    new_embeds = emb_model(rounded_tokens).view(old_shape).to(old_device)\n",
    "\n",
    "    return new_embeds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************standard sample**************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:34<00:00, 58.53it/s]\n",
      "100%|██████████| 1/1 [00:34<00:00, 34.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['517 183 293 17 604 595 422 411 426 659 247 279 339 978 324 44 322', '67 99 56']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate 1 sample for each data\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "each_sample_list = []\n",
    "\n",
    "for _, batch in enumerate(tqdm(dev_dataloader)):\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden_states = model.encoder(\n",
    "            input_ids=batch['src_input_ids'].to(device), \n",
    "            attention_mask=batch['src_attention_mask'].to(device),\n",
    "        ).last_hidden_state  # [bs, seq_len, hz]\n",
    "\n",
    "    if config.pred_len:\n",
    "        with torch.no_grad():\n",
    "            length_out = model.get_pred_len(\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                src_masks=batch['src_attention_mask'].to(device),\n",
    "                normalize=True,\n",
    "            )  # [bs, max_pos_len]\n",
    "            pred_lengs = length_out.max(-1)[1]  # [bs,], max return tuple(value, indices)\n",
    "\n",
    "        tgt_attention_mask = []\n",
    "        for len_item in pred_lengs:\n",
    "            tgt_attention_mask.append([1] * len_item + [0] * (max(pred_lengs) - len_item))\n",
    "        tgt_attention_mask = torch.tensor(tgt_attention_mask).long()\n",
    "        \n",
    "        input_shape = (\n",
    "            tgt_attention_mask.shape[0], tgt_attention_mask.shape[1], config.in_channels,\n",
    "        )\n",
    "    else:\n",
    "        pred_lengs, tgt_attention_mask = None, None\n",
    "        input_shape = (\n",
    "            batch['src_input_ids'].shape[0], config.tgt_len, config.in_channels,\n",
    "        )\n",
    "\n",
    "    model_kwargs = {'src_attention_mask': batch['src_attention_mask'].to(device),\n",
    "                    'tgt_attention_mask': tgt_attention_mask,\n",
    "                    'encoder_hidden_states': encoder_hidden_states,}\n",
    "\n",
    "    sample = sample_fn(\n",
    "        model,\n",
    "        input_shape,\n",
    "        clip_denoised=config.clip_denoised,\n",
    "        # \"Freeze\" some parameters for easy recall.\n",
    "        denoised_fn=partial(denoised_fn_round,\n",
    "                            config, emb_model.to(device)),\n",
    "        progress=True,\n",
    "        model_kwargs=model_kwargs,\n",
    "        pred_lengs=pred_lengs,\n",
    "        top_p=-1.0,\n",
    "    )\n",
    "\n",
    "\n",
    "    logits = model.get_logits(sample)  # (bs, seq_len, vocab_size)\n",
    "    sample_id_tensor = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    \n",
    "    each_sample_list.extend(tokenizer.batch_decode(sample_id_tensor, skip_special_tokens=True))\n",
    "\n",
    "    print(tokenizer.batch_decode(sample_id_tensor, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAfVUlEQVR4nO3dXWxc1d3v8d+esWfsENvBhNhxcULCW1peUimQkANUUKy8XEQEcgGIi4AiKlEHKVgIKecUAio6FlSiiCoN56Il5YKXckEQXKSihjiqmgQRHtQHqScnidwnyQl2IOeJHTvxeGb2PheUeeSSgGf9bf4zzvcjjWTP7OW1Zs+a+c327Fn/KEmSRAAAfM9S3gMAAFyYCCAAgAsCCADgggACALgggAAALgggAIALAggA4IIAAgC4qPEewL+K41jHjx9XQ0ODoijyHg4AoExJkuj06dNqa2tTKnX+45yKC6Djx4+rvb3dexgAAKOjR4/qsssuO+/tFRdADQ0NkqR//7hFDTPL/w9hXWS7S9moNrjtn85kTX3/+9nw4P3DJ/8tuG3m/2aC20pSnA1fzSk1Zupa9V+EHyWPNdj6zgyFt607FQe3jYrh/UqSDKtvzTxyxtR16mwhuG2UC58syQzbczMqGB6v0Zyp72TwdHjf9XWmvpUJez0sxGPa9R//q/R6fj4VF0Bf/9utYWZKjQ0hAWT7WCtraD8jnTb1XZcOD7+UYaKl62wBJEMApY2fQqYz4QGUtr0mKW3Ybena8Be0b/mPxsQYAqimJnzckpRK54PbRunwxzoxPthRYggg4+OVpMIDLEoZJ3kq/DVJ0nd+jDJlJyFs3bpVl19+uerq6rRs2TJ99NFHU9UVAKAKTUkAvfnmm+rq6tKWLVv0ySefaPHixVq5cqVOnDgxFd0BAKrQlATQCy+8oIcfflgPPfSQfvSjH+nll1/WjBkz9Pvf/34qugMAVKFJD6CxsTHt379fHR0d/9VJKqWOjg7t2bPnG9vncjkNDQ2NuwAApr9JD6Avv/xSxWJRLS0t465vaWlRf3//N7bv7u5WU1NT6cIp2ABwYXBfCWHz5s0aHBwsXY4ePeo9JADA92DST8OePXu20um0BgYGxl0/MDCg1tbWb2yfzWaVzRpPFQQAVJ1JPwLKZDJasmSJenp6StfFcayenh4tX758srsDAFSpKfkialdXl9avX68bb7xRS5cu1YsvvqiRkRE99NBDU9EdAKAKTUkA3Xvvvfriiy/01FNPqb+/Xz/+8Y+1c+fOb5yYAAC4cE3ZUjwbN27Uxo0bp+rPAwCqXMWtBfe1/zOW0UVj5X9E1VZz1tRvv2Ghx6NjPzD1fSY2LC5mqFyRpMPXBjP37XgeZmJbus/cPlTRtjyXotiwplqt8U7XGJ5gcfjLVVJjm2iW9qmCbfXYKGN4XQhcTPRrSTasfVKc2Np57qdhAwAuTAQQAMAFAQQAcEEAAQBcEEAAABcEEADABQEEAHBBAAEAXBBAAAAXBBAAwAUBBABwQQABAFwQQAAAFwQQAMAFAQQAcFGx9YCOFi7WjEL5tUeG4jOmfk/H9cFtj+QuMfX9xVhDeONieI2XaGKlO87PUk7IWIrIVE/Ise+41lBEySoKv+NxrfE964zw+jSWSkTmOkZx+D5L6mw1eaJ8XXjf9VlT36F1kJLixGogcQQEAHBBAAEAXBBAAAAXBBAAwAUBBABwQQABAFwQQAAAFwQQAMAFAQQAcEEAAQBcEEAAABcEEADABQEEAHBBAAEAXFRsOYaPhxcoq/KXMU8Zlpq3tj+RM5RTkHRk+OLgtqlhw3Lzia00QGSoa5AqGPs2lJIwl6EwiF2feeH7vDDDVtYgnQ7vOzGUgijU23Z4VAifLOmcre90Jrx9nLE9XnEmbJ8XJvi85ggIAOCCAAIAuCCAAAAuCCAAgAsCCADgggACALgggAAALgggAIALAggA4IIAAgC4IIAAAC4IIACACwIIAOCCAAIAuCCAAAAuKrYe0MHhOapNMmW3qzEWeSkk4Zl88uwMW9/F8NodSTq8Jk+SstVQkmGXxzW2vovZ8PoycdbUtZJceNuisW+LqBjeNs7Y6jclqfA5nh619BveVrLVIlLKWPOqGP4EK2Zt9YBCJRH1gAAAFYwAAgC4IIAAAC4IIACACwIIAOCCAAIAuCCAAAAuCCAAgAsCCADgggACALgggAAALgggAIALAggA4IIAAgC4qNhyDP/7cJtS9XXlNzRWFlAcvnR6VLQtu24pa1AzHP5eIjLusyhv2Ge26hlKDDM4Khj7Nqx0bykj4enMJbbl/S2Pd6oQ3ndsfKVLGUpYRLHtCZa6OHzw1vudBJaSKI5N7IHmCAgA4IIAAgC4IIAAAC4mPYCefvppRVE07rJo0aLJ7gYAUOWm5CSEa6+9Vn/+85//q5Oaij3XAQDgZEqSoaamRq2trVPxpwEA08SUfAZ08OBBtbW1aeHChXrggQd05MiR826by+U0NDQ07gIAmP4mPYCWLVum7du3a+fOndq2bZv6+vp022236fTp0+fcvru7W01NTaVLe3v7ZA8JAFCBoiRJrF/d/FanTp3S/Pnz9cILL2jDhg3fuD2XyymXy5V+HxoaUnt7uy77zTN8EbUMNafD30ukjF/IVOL3RdSas+Ftixlb3+ncd29zPqm8rW8vtadtTzDbF1HD21b1F1ENc8Xvi6ij+rc3/ocGBwfV2Nh43u2m/OyAWbNm6eqrr9ahQ4fOeXs2m1U2m53qYQAAKsyUfw9oeHhYhw8f1ty5c6e6KwBAFZn0AHr88cfV29urf/zjH/rrX/+qu+++W+l0Wvfff/9kdwUAqGKT/i+4Y8eO6f7779fJkyd16aWX6tZbb9XevXt16aWXTnZXAIAqNukB9MYbb0z2nwQATEMVu0RBaiStVLH85dcT4z8VLWfaWJbnl2Q6Cy4ynKVjOIlNkpQ2nKVjLYlgOevROlcs+9x0tqbx8bLM07jW1rfprEfD/S7U2XZaOu/3gKUsryvWuRL4HJnoiX8sRgoAcEEAAQBcEEAAABcEEADABQEEAHBBAAEAXBBAAAAXBBAAwAUBBABwQQABAFwQQAAAFwQQAMAFAQQAcEEAAQBcEEAAABeVWw9oLFIqVX4xi7jWUrfDpmbYVnzDUqclVQjvOz0a3q9kqxFjri9jeLgttZ8kKc6Et3WrJSTbPktqjAVmCuGdW2r6RBMtUHMexUx43ylTLSGZavrE1hplgX1PtI4QR0AAABcEEADABQEEAHBBAAEAXBBAAAAXBBAAwAUBBABwQQABAFwQQAAAFwQQAMAFAQQAcEEAAQBcEEAAABcEEADARcWWYyg055WqL38t8ajGtvR5bChrUKz3y/MkMqy7nhiX2K9SlnIKklQzEt42nQtvO9Gl7s//B8Kbpkdtzy9LGYqUoZSDpdSJZCvnYC0FUXM2vH1c6/PcLkywBAVHQAAAFwQQAMAFAQQAcEEAAQBcEEAAABcEEADABQEEAHBBAAEAXBBAAAAXBBAAwAUBBABwQQABAFwQQAAAFwQQAMAFAQQAcFGx9YDq+zJKZ8sv2JIqGDt2LI1TrWV5oji8rbW2jaV9lLf1bZkrhfrwttZ9FhnK01jry1jq8ljmmZXldSWuse0zSw0l61xJBfZdzE2sY46AAAAuCCAAgAsCCADgggACALgggAAALgggAIALAggA4IIAAgC4IIAAAC4IIACACwIIAOCCAAIAuCCAAAAuCCAAgIuKLceQKoYtBW5Z7l3yXfLdunR6qNAl10sMq81bSgNIkqq1FITjEvuec9zSt6nsh7HUSez5Sun4/IoDX08n2o4jIACACwIIAOCCAAIAuCg7gHbv3q01a9aora1NURRpx44d425PkkRPPfWU5s6dq/r6enV0dOjgwYOTNV4AwDRRdgCNjIxo8eLF2rp16zlvf/755/XSSy/p5Zdf1r59+3TRRRdp5cqVGh0dNQ8WADB9lH1ux+rVq7V69epz3pYkiV588UX94he/0F133SVJevXVV9XS0qIdO3bovvvus40WADBtTOpnQH19ferv71dHR0fpuqamJi1btkx79uw5Z5tcLqehoaFxFwDA9DepAdTf3y9JamlpGXd9S0tL6bZ/1d3draamptKlvb19MocEAKhQ7mfBbd68WYODg6XL0aNHvYcEAPgeTGoAtba2SpIGBgbGXT8wMFC67V9ls1k1NjaOuwAApr9JDaAFCxaotbVVPT09peuGhoa0b98+LV++fDK7AgBUubLPghseHtahQ4dKv/f19enTTz9Vc3Oz5s2bp02bNunZZ5/VVVddpQULFujJJ59UW1ub1q5dO5njBgBUubID6OOPP9Ydd9xR+r2rq0uStH79em3fvl1PPPGERkZG9LOf/UynTp3Srbfeqp07d6qurm7yRg0AqHpRkiTW9Ygn1dDQkJqamnTNpv+pdLb80KrmlYJDV561Mq+G7ciyyrF1hWPLfrOshu05bsvq45JsKzs7roZtGbeV6wr9gfe7mBvVgZf+uwYHB7/1c333s+AAABemiq0HlBlKlM6Uf3AW19jeqqQK4QeE1ndZiWHsxayhX+PbkFQhvK25zoph7JZxS7b9lljut7W2jeFIOzHMM8mxDpL1/zyOR0DmsTuY6MPMERAAwAUBBABwQQABAFwQQAAAFwQQAMAFAQQAcEEAAQBcEEAAABcEEADABQEEAHBBAAEAXBBAAAAXBBAAwAUBBABwUbHlGOLaSFFt+WugR0Xb2uXFTPi669bCUabSBJa7bS0jYSkyZi2GZylIZywAaCnnUJgR3ta6z0ztjaUB3EoqWEuOjIW3jWuNfRvmmbnUSuhcyU/w7wf+eQAATAggAIALAggA4IIAAgC4IIAAAC4IIACACwIIAOCCAAIAuCCAAAAuCCAAgAsCCADgggACALgggAAALgggAICLii3HUHs6UTpT/vrr6byxHINl6XRjWQNLOYZiNrzz2LjEvol1eX9DCYy0saxBYinnYLjfpn6NfVtLCygK79ytlIOkJGV4ftUYX5PqTM1tAoce106sIUdAAAAXBBAAwAUBBABwQQABAFwQQAAAFwQQAMAFAQQAcEEAAQBcEEAAABcEEADABQEEAHBBAAEAXBBAAAAXBBAAwAUBBABwUbH1gKLEVDqkKkXG+jTBDDV1JCmxzCLrY2yowWTd35b6Tam8rW8LU10dc+eWB8zvBSEOqE1WYq1FZKj/ZJ3joc/tibbjCAgA4IIAAgC4IIAAAC4IIACACwIIAOCCAAIAuCCAAAAuCCAAgAsCCADgggACALgggAAALgggAIALAggA4IIAAgC4qNhyDLXDsWpqy68TECW2tc9rDW0LWVueW5bJL2YNHWcMS+RLSucMjY1L1Rdm+PVtWereVBLB9nCZ3nbGWdtOs1RjSGoMfce2nWapBJGkjPvM8Cod5a2TJcxES5VwBAQAcEEAAQBcEEAAABdlB9Du3bu1Zs0atbW1KYoi7dixY9ztDz74oKIoGndZtWrVZI0XADBNlB1AIyMjWrx4sbZu3XrebVatWqXPP/+8dHn99ddNgwQATD9ln1+xevVqrV69+lu3yWazam1tDR4UAGD6m5LPgHbt2qU5c+bommuu0SOPPKKTJ0+ed9tcLqehoaFxFwDA9DfpAbRq1Sq9+uqr6unp0XPPPafe3l6tXr1axeK5vzTR3d2tpqam0qW9vX2yhwQAqECT/kXU++67r/Tz9ddfrxtuuEFXXHGFdu3apTvvvPMb22/evFldXV2l34eGhgghALgATPlp2AsXLtTs2bN16NChc96ezWbV2Ng47gIAmP6mPICOHTumkydPau7cuVPdFQCgipT9L7jh4eFxRzN9fX369NNP1dzcrObmZj3zzDNat26dWltbdfjwYT3xxBO68sortXLlykkdOACgupUdQB9//LHuuOOO0u9ff36zfv16bdu2TX/729/0hz/8QadOnVJbW5tWrFihX/7yl8pmLatlAgCmm7ID6Pbbb1fyLStO/+lPfzINCABwYWAtOACAi4qtB9Tw95OqSQf8264mbeu4EF7kJW6sN3UdZ8LHHrWH9z1WCG4qSUrlw9tOtG7I+RSz4fVOas7a6rQUZoT3baljZKlNI0mJ4SmSn2UogiRJNeXX+Co1nRE+UQsjlkpfMo07ZWgrSTNmhhfcGj2bMfUdF8PmeHJmdELbcQQEAHBBAAEAXBBAAAAXBBAAwAUBBABwQQABAFwQQAAAFwQQAMAFAQQAcEEAAQBcEEAAABcEEADABQEEAHBBAAEAXFRsOYa4oU5xQDmGqGBb+lyZ8F1SrDcu+W54O5A4tZUkhVclMJcWsDDfbwPT/bbuM0P7qGB4sCUlUXj70NIAkiRLW0mKwyeL8RVJ+Xx4/Yw4tt3vOLDvuDCxdhwBAQBcEEAAABcEEADABQEEAHBBAAEAXBBAAAAXBBAAwAUBBABwQQABAFwQQAAAFwQQAMAFAQQAcEEAAQBcEEAAABcVW44hdTavVDogHwtFW8c14Uufp3Lhba3S+Uxw25RxiX2LxK9rpQq29qaZZlij31rCIjZUDUnSxs4Nb3lThr7jGltRhKgmvO8obeu7PpsPbpsYn2BxKux+x/HEnlwcAQEAXBBAAAAXBBAAwAUBBABwQQABAFwQQAAAFwQQAMAFAQQAcEEAAQBcEEAAABcEEADABQEEAHBBAAEAXBBAAAAXBBAAwEXF1gNKalJKQuoBZYx3qRBeuyPO+u3OYm143Y/E+jbEWCLGi/V+x4byT5GhRExiLDtl6TuKbfVlEkMRpWLe8IAVbA92krLU9LH1ncuHv64U8sbJEvjcjosTmyccAQEAXBBAAAAXBBAAwAUBBABwQQABAFwQQAAAFwQQAMAFAQQAcEEAAQBcEEAAABcEEADABQEEAHBBAAEAXBBAAAAXFVuOITrWryjKlN0uKVqWTZeUzwc3zbS1mrpOasKXTs82NAe3jWts06Bm1FDCosa2vH++Ibx9etTUtVJ14W3jWkPH1vIXll1+UcHUdW1d+POr4aLwB+w//3NmcFtJmjEzF9y2PhN+nyXpmuYTwW0/P9No6vt0LhvUrliX05EJbMcREADABQEEAHBBAAEAXJQVQN3d3brpppvU0NCgOXPmaO3atTpw4MC4bUZHR9XZ2alLLrlEM2fO1Lp16zQwMDCpgwYAVL+yAqi3t1ednZ3au3ev3n//feXzea1YsUIjIyOlbR577DG9++67euutt9Tb26vjx4/rnnvumfSBAwCqW1mnP+3cuXPc79u3b9ecOXO0f/9+/eQnP9Hg4KB+97vf6bXXXtNPf/pTSdIrr7yiH/7wh9q7d69uvvnmyRs5AKCqmT4DGhwclCQ1N391CvD+/fuVz+fV0dFR2mbRokWaN2+e9uzZc86/kcvlNDQ0NO4CAJj+ggMojmNt2rRJt9xyi6677jpJUn9/vzKZjGbNmjVu25aWFvX395/z73R3d6upqal0aW9vDx0SAKCKBAdQZ2enPvvsM73xxhumAWzevFmDg4Oly9GjR01/DwBQHYK+Ar9x40a999572r17ty677LLS9a2trRobG9OpU6fGHQUNDAyotfXcqwRks1lls2HftgUAVK+yjoCSJNHGjRv19ttv64MPPtCCBQvG3b5kyRLV1taqp6endN2BAwd05MgRLV++fHJGDACYFso6Aurs7NRrr72md955Rw0NDaXPdZqamlRfX6+mpiZt2LBBXV1dam5uVmNjox599FEtX76cM+AAAOOUFUDbtm2TJN1+++3jrn/llVf04IMPSpJ+/etfK5VKad26dcrlclq5cqV++9vfTspgAQDTR1kBlCTfvQxvXV2dtm7dqq1btwYPCgAw/bEWHADARcXWAwpWLNrap8IzOUnb8jwqhI89KhqKxNhK8phE1to2lrttLB2VMk61UNZxW3Z5UvSbLClD1zW1tgerWAx/bqdTtgfsoprwWkTZtK1+02g6LCKiCT45OAICALgggAAALgggAIALAggA4IIAAgC4IIAAAC4IIACACwIIAOCCAAIAuCCAAAAuCCAAgAsCCADgggACALgggAAALiq2HENx8LSiqLbsdlEmY+o3yYUvfV4zfMbUt9Lp4KaWcgypvK0mQsqw4ntsXd3f0D6aQIHFb5NETqUJHMtnpOtsZQ3S6fB9Xl+bD247lKoLbitJM+rCXxcyads+m5k29G2sGZIOrJeSTLAdR0AAABcEEADABQEEAHBBAAEAXBBAAAAXBBAAwAUBBABwQQABAFwQQAAAFwQQAMAFAQQAcEEAAQBcEEAAABcEEADABQEEAHBRsfWA0hfPUjpVfm2f5Oyoqd/UrKbwxlljLaJM+fWPSm3T4UViAkt+lBSz4X3H4SWQvmIYu7WeT2J49sThD7Xr28a4YNtnhSj8AT89mg3vN2+baCNnw/tOEts++48zzcFt/9/oDFPf+ThsshUn2I4jIACACwIIAOCCAAIAuCCAAAAuCCAAgAsCCADgggACALgggAAALgggAIALAggA4IIAAgC4IIAAAC4IIACAi4pbDTtJvlreuJCMSXFI+zFT/1ESvrxyFOdMfSfFgDv8T4VC+CrghbzfNEiKtvbFXPh7qOKYbRnwYi58leOiZeF02+LKJrFxtfkoHz7Hi1H48ys+Y9tpxUIhvG2cN/Wdz4S/phXO2F6TJrqq9Tfa/bPf5DteT6Pku7b4nh07dkzt7e3ewwAAGB09elSXXXbZeW+vuACK41jHjx9XQ0ODonPUaxkaGlJ7e7uOHj2qxsZGhxFWH/ZZ+dhn5WOflW+67rMkSXT69Gm1tbUplTr/UVTF/QsulUp9a2J+rbGxcVo9YN8H9ln52GflY5+Vbzrus6am7y7uyUkIAAAXBBAAwEXVBVA2m9WWLVuUzYbXaL/QsM/Kxz4rH/usfBf6Pqu4kxAAABeGqjsCAgBMDwQQAMAFAQQAcEEAAQBcVF0Abd26VZdffrnq6uq0bNkyffTRR95DqlhPP/20oigad1m0aJH3sCrK7t27tWbNGrW1tSmKIu3YsWPc7UmS6KmnntLcuXNVX1+vjo4OHTx40GewFeK79tmDDz74jXm3atUqn8FWgO7ubt10001qaGjQnDlztHbtWh04cGDcNqOjo+rs7NQll1yimTNnat26dRoYGHAa8fenqgLozTffVFdXl7Zs2aJPPvlEixcv1sqVK3XixAnvoVWsa6+9Vp9//nnp8pe//MV7SBVlZGREixcv1tatW895+/PPP6+XXnpJL7/8svbt26eLLrpIK1eu1OiobVHOavZd+0ySVq1aNW7evf7669/jCCtLb2+vOjs7tXfvXr3//vvK5/NasWKFRkZGSts89thjevfdd/XWW2+pt7dXx48f1z333OM46u9JUkWWLl2adHZ2ln4vFotJW1tb0t3d7TiqyrVly5Zk8eLF3sOoGpKSt99+u/R7HMdJa2tr8qtf/ap03alTp5JsNpu8/vrrDiOsPP+6z5IkSdavX5/cddddLuOpBidOnEgkJb29vUmSfDWnamtrk7feequ0zd///vdEUrJnzx6vYX4vquYIaGxsTPv371dHR0fpulQqpY6ODu3Zs8dxZJXt4MGDamtr08KFC/XAAw/oyJEj3kOqGn19ferv7x8355qamrRs2TLm3HfYtWuX5syZo2uuuUaPPPKITp486T2kijE4OChJam5uliTt379f+Xx+3DxbtGiR5s2bN+3nWdUE0JdffqlisaiWlpZx17e0tKi/v99pVJVt2bJl2r59u3bu3Klt27apr69Pt912m06fPu09tKrw9bxizpVn1apVevXVV9XT06PnnntOvb29Wr16tYpFY/GnaSCOY23atEm33HKLrrvuOklfzbNMJqNZs2aN2/ZCmGcVtxo2Js/q1atLP99www1atmyZ5s+frz/+8Y/asGGD48gwnd13332ln6+//nrdcMMNuuKKK7Rr1y7deeedjiPz19nZqc8++4zPYv+pao6AZs+erXQ6/Y0zQwYGBtTa2uo0quoya9YsXX311Tp06JD3UKrC1/OKOWezcOFCzZ49+4Kfdxs3btR7772nDz/8cFzJmdbWVo2NjenUqVPjtr8Q5lnVBFAmk9GSJUvU09NTui6OY/X09Gj58uWOI6sew8PDOnz4sObOnes9lKqwYMECtba2jptzQ0ND2rdvH3OuDMeOHdPJkycv2HmXJIk2btyot99+Wx988IEWLFgw7vYlS5aotrZ23Dw7cOCAjhw5Mu3nWVX9C66rq0vr16/XjTfeqKVLl+rFF1/UyMiIHnroIe+hVaTHH39ca9as0fz583X8+HFt2bJF6XRa999/v/fQKsbw8PC4d+Z9fX369NNP1dzcrHnz5mnTpk169tlnddVVV2nBggV68skn1dbWprVr1/oN2tm37bPm5mY988wzWrdunVpbW3X48GE98cQTuvLKK7Vy5UrHUfvp7OzUa6+9pnfeeUcNDQ2lz3WamppUX1+vpqYmbdiwQV1dXWpublZjY6MeffRRLV++XDfffLPz6KeY92l45frNb36TzJs3L8lkMsnSpUuTvXv3eg+pYt17773J3Llzk0wmk/zgBz9I7r333uTQoUPew6ooH374YSLpG5f169cnSfLVqdhPPvlk0tLSkmSz2eTOO+9MDhw44DtoZ9+2z86cOZOsWLEiufTSS5Pa2tpk/vz5ycMPP5z09/d7D9vNufaVpOSVV14pbXP27Nnk5z//eXLxxRcnM2bMSO6+++7k888/9xv094RyDAAAF1XzGRAAYHohgAAALgggAIALAggA4IIAAgC4IIAAAC4IIACACwIIAOCCAAIAuCCAAAAuCCAAgAsCCADg4v8DcsLctcMhCGwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = model.transformer_blocks[0].attn1.attention_probs\n",
    "# average over the 8 attention heads\n",
    "w = w.mean(0)\n",
    "# w=w[]\n",
    "# print heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(w.detach().cpu().numpy())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Src: ['37 63 89 28 43 57 10 33', '34', '44 31', '67 99 56', '517 183 293 17 604 595 422 411 426 659 247 279 339 978 324 44 322 67 402']\n",
      "Tgt: ['37 63 89 28 43 57 10 33', '34', '44 31', '67 99 56', '517 183 293 17 604 595 422 411 426 659 247 279 339 978 324 44 322 67 402']\n",
      "Sample: ['37 63 89 28 43 57 10 33', '34', '44 31', '67 99 56', '517 183 293 17 604 595 422 411 426 659 247 279 339 978 324 44 322']\n"
     ]
    }
   ],
   "source": [
    "print(\"Src:\", [d[\"src\"] for d in data_piece])\n",
    "print(\"Tgt:\", [d[\"tgt\"] for d in data_piece])\n",
    "print(\"Sample:\", each_sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "0: 517 -> 517\n",
      "1: 183 -> 183\n",
      "2: 293 -> 293\n",
      "3: 17 -> 17\n",
      "4: 604 -> 604\n",
      "5: 595 -> 595\n",
      "6: 422 -> 422\n",
      "7: 411 -> 411\n",
      "8: 426 -> 426\n",
      "9: 659 -> 659\n",
      "10: 247 -> 247\n",
      "11: 279 -> 279\n",
      "12: 339 -> 339\n",
      "13: 978 -> 978\n",
      "14: 324 -> 324\n",
      "15: 44 -> 44\n",
      "16: 322 -> 322\n",
      "----------\n",
      "0: 67 -> 67\n",
      "1: 99 -> 99\n",
      "2: 56 -> 56\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for tgt, gen in zip([d[\"tgt\"].split(\" \") for d in data_piece], each_sample_list):\n",
    "    print(f\"----------\")\n",
    "    i = 0\n",
    "    for t, g in zip(tgt, gen.split(\" \")):\n",
    "        print(f\"{i}: {t} -> {g}\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
