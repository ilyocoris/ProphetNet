{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gcilleru/ProphetNet/AR-diffusion/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import hydra\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import load_states_from_checkpoint\n",
    "from data_utils.s2s_dataset import load_jsonl_data, S2S_dataset\n",
    "from model_utils.create_model import create_model, create_gaussian_diffusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0 reporting in!\n"
     ]
    }
   ],
   "source": [
    "# fake torch distributed\n",
    "from torch import distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "def initialize_distributed():\n",
    "    if not dist.is_initialized():\n",
    "        # Initialize the distributed environment\n",
    "        dist.init_process_group(backend='gloo')  # 'gloo' is suitable for local development\n",
    "\n",
    "# Call the initialization function\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1' \n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '1235'\n",
    "initialize_distributed()\n",
    "\n",
    "# Now you can use distributed functions safely\n",
    "rank = dist.get_rank()\n",
    "print(f\"Rank {rank} reporting in!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2897881/930210726.py:1: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  hydra.initialize(config_path=\".\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hydra.initialize()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydra.initialize(config_path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vanilla_CrossAttention_Diffusion_LM(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (input_up_proj): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=1536, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  )\n",
       "  (output_down_proj): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1536, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=1536, out_features=128, bias=True)\n",
       "  )\n",
       "  (word_embedding): Embedding(30522, 128)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=128, out_features=30522, bias=True)\n",
       "  (time_trans): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-5): 6 x BasicTransformerBlock(\n",
       "      (attn1): CrossAttention(\n",
       "        (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (to_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (to_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropatt): Dropout(p=0.1, inplace=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=768, out_features=6144, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (attn2): CrossAttention(\n",
       "        (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (to_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (to_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropatt): Dropout(p=0.1, inplace=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vanilla model\n",
    "config = hydra.compose(config_name=\"config_recipes.yaml\")\n",
    "model = create_model(config, config.vocab_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = hydra.compose(config_name=\"config_recipes.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA_VISIBLE_DEVICES=6 torchrun --nproc_per_node=1 --nnodes=1 --master_port 29501 generate.py \\\n",
    "# model.name='bert-base-uncased' batch_size=128 \\\n",
    "# exp.name=$FILE_NAME load_step=$STEP \\\n",
    "# data.name=$DATA_NAME data.path=$DATA_PATH tgt_len=32 max_pos_len=32 num_samples=50 \\\n",
    "# intermediate_size=2048 num_attention_heads=8 dropout=0.2 \\\n",
    "# in_channels=128 out_channels=128 time_channels=128 \\\n",
    "# skip_sample=False gen_timesteps=2000 \\\n",
    "# schedule_sampler='uniform' time_att=True att_strategy='txl' load_from_ema=False prediction=True \\\n",
    "# fix_encoder=True model.custom_denoiser=True model.denoiser_layers=1 \\\n",
    "\n",
    "\n",
    "config\n",
    "\n",
    "config.exp.name=\"d1_uni\"\n",
    "config.load_step=20000\n",
    "config.data.name=\"reverse\"\n",
    "config.data.path=\"data/raw/sequence\"\n",
    "\n",
    "config.model.name = 'bert-base-uncased'\n",
    "config.tgt_len=128\n",
    "config.max_pos_len=512\n",
    "config.num_samples=1 # how many generations for each sample\n",
    "\n",
    "config.intermediate_size=2048\n",
    "config.num_attention_heads=8\n",
    "config.dropout=0.2\n",
    "config.in_channels=128\n",
    "config.out_channels=128\n",
    "config.time_channels=128\n",
    "\n",
    "config.skip_sample=False\n",
    "config.gen_timesteps=2000\n",
    "config.schedule_sampler='uniform'\n",
    "config.time_att=True\n",
    "config.att_strategy='txl'\n",
    "\n",
    "config.load_from_ema=False\n",
    "config.prediction=True\n",
    "\n",
    "# config.fix_encoder=True\n",
    "config.model.custom_denoiser=False\n",
    "config.model.denoiser_layers=1\n",
    "\n",
    "config.batch_size = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate path:  ./my_output/recipes/ebert_d6_c128/0_un_100\n",
      "Exp dir:  ./my_output/recipes/ebert_d6_c128\n"
     ]
    }
   ],
   "source": [
    "config.exp.dir = os.path.join(config.exp.root, config.data.name, config.exp.name)\n",
    "generate_path = os.path.join(config.exp.dir, str(config.load_step))\n",
    "if config.load_from_ema:\n",
    "    generate_path += ('_ema_' + str(config.ema_rate))\n",
    "if config.clip_denoised:\n",
    "    generate_path += '_clip_denoised_'\n",
    "if config.infer_self_condition:\n",
    "    generate_path += '_selfcond_'\n",
    "if config.skip_sample:\n",
    "    generate_path += '_skip_'\n",
    "if config.ddim_sample:\n",
    "    generate_path += '_ddim_'\n",
    "\n",
    "if config.schedule_sampler == 'xy_uniform':\n",
    "    generate_path += ('_xy_' + str(config.gen_timesteps))\n",
    "else:\n",
    "    generate_path += ('_un_' + str(config.skip_timestep))\n",
    "\n",
    "print(\"Generate path: \", generate_path)\n",
    "print(\"Exp dir: \", config.exp.dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.tokenizer)\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model from:  my_output/recipes/ebert_d6_c128/model/model_checkpoint-30000\n"
     ]
    }
   ],
   "source": [
    "# if config.load_from_ema:\n",
    "#     eval_model_path = os.path.join(\n",
    "#         config.exp.dir, 'model', f'ema_{config.ema_rate}_checkpoint-{config.load_step}')\n",
    "# else:\n",
    "#     eval_model_path = os.path.join(\n",
    "#         config.exp.dir, 'model', f'model_checkpoint-{config.load_step}')\n",
    "# eval_model_path = \"data/models/GENIE_ckpt-500w\"\n",
    "# eval_model_path = \"my_output/reverse/d1_uni/model/model_checkpoint-80000\"\n",
    "# eval_model_path = \"my_output/reverse_words/revseqw_l1_te/model/model_checkpoint-50000\"\n",
    "eval_model_path = \"my_output/recipes/ebert_d6_c128/model/model_checkpoint-30000\"\n",
    "print(\"Load model from: \", eval_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion = create_gaussian_diffusion(config)\n",
    "model = create_model(config, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_saved_state = load_states_from_checkpoint(eval_model_path, dist.get_rank())\n",
    "model.load_state_dict(model_saved_state.model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vanilla_CrossAttention_Diffusion_LM(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (input_up_proj): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=1536, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  )\n",
       "  (output_down_proj): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1536, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=1536, out_features=128, bias=True)\n",
       "  )\n",
       "  (word_embedding): Embedding(30522, 128)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=128, out_features=30522, bias=True)\n",
       "  (time_trans): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-5): 6 x BasicTransformerBlock(\n",
       "      (attn1): CrossAttention(\n",
       "        (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (to_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (to_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropatt): Dropout(p=0.1, inplace=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GEGLU(\n",
       "            (proj): Linear(in_features=768, out_features=6144, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (attn2): CrossAttention(\n",
       "        (to_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (to_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (to_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropatt): Dropout(p=0.1, inplace=False)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample text from random noise\n",
    "if config.ddim_sample:\n",
    "    sample_fn = (diffusion.ddim_sample_loop)\n",
    "else:\n",
    "    sample_fn = (diffusion.p_sample_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = model.word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_piece = [\n",
    "    {\n",
    "        \"src\": \"Mandarin Spinach Salad Ingredients:  [MASK]  Directions: -Mix 2 tablespoons sugar with small amount of water over low heat until sugar has melted. -Add almonds and stir until well coated. -Cook, then break apart. -Store at room temperature. -In large bowl, place lettuce, spinach, celery and onions. Immediately before serving, toss with Sweet-Sour Dressing. -Then add mandarin oranges and almonds. -Don't add Sweet-Sour Dressing until ready to serve; it will make the salad soggy.\",\n",
    "        \"tgt\":\"1/4 c. sliced almonds 1 head lettuce, torn into small pieces 1 bag spinach, torn into small pieces 2 medium stalks celery, chopped 2 green onions, thinly sliced 1 (11 oz.) can mandarin oranges, drained hard-boiled eggs (optional)\"\n",
    "    },\n",
    "    {\n",
    "        \"src\":\"Prune Bread Ingredients: 1 1/2 cups prunes dried, - without pits 2 cups all purpose flour 3/4 cup sugar 1 teaspoon baking soda 1/2 teaspoon salt 1 egg 2 tablespoons shortening melted Directions: -In medium saucepan, combine prunes with 1 1/2 cups water, bring to boiling. Reduce heat, and simmer, covered. 10 minutes. -Preheat oven 350 degrees. -[MASK] -Let cool in pan on wire rack 10 minutes. Remove from pan: cool completely on rack. Wrap in plastic film or foil, and refrigerate overnight before slicing.\",\n",
    "        \"tgt\":\"Sift flour with sugar, baking soda and salt; set aside. -Drain prunes, reserving liquid. Chop prunes or place in food processor and give a few good chops. -Add to reserved liquid, and measure. Add water if needed to make 2 cups. -In a large bowl combine egg and shortening: with electric mixer at medium speed, beat well. Add prune mixture: beat until well blended. -Add flour mixture: beat at low speed just until smooth. Turn into prepared pan. -Bake 50-60 minutes, or until cake tester inserted in center comes out clean.\"\n",
    "    },\n",
    "    {\n",
    "        \"src\":\"No Bake Chocolate Cookies Ingredients: 1/4 lb. margarine 1/2 c. milk 1/2 c. chocolate bits 2 c. sugar 3 c. oatmeal 1/2 c. peanut butter 1 tsp. vanilla Directions: -[MASK] -Remove from heat and add oatmeal, peanut butter and vanilla. -Cool slightly and drop by teaspoons on waxed paper.\",\n",
    "        \"tgt\": \"Bring butter, milk, chocolate and sugar to a boil in a large saucepan. Boil and stir continuously for 1 minute.\"\n",
    "    }\n",
    "    # {\n",
    "    #     \"src\": \"non mcdowell explosion\",\n",
    "    #     \"tgt\": \"explosion mcdowell non\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\": \"angels aldo ##oulos slovenian\",\n",
    "    #     \"tgt\": \"slovenian ##oulos aldo angels\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\": \"eased mention ##bine outcomes ##efe aldo ##糹 jumped ##claim weakness\",\n",
    "    #     \"tgt\": \"weakness ##claim jumped ##糹 ##efe outcomes ##bine mention ##aldo eased\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\": \"37 63 89 28 43 57 10 33\",\n",
    "    #     \"tgt\": \"33 10 57 43 28 89 63 37\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\":\"34\",\n",
    "    #     \"tgt\":\"34\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\":\"44 31\",\n",
    "    #     \"tgt\":\"31 44\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"src\":\"67 99 56\",\n",
    "    #     \"tgt\":\"56 99 67\"\n",
    "    # }\n",
    "]\n",
    "\n",
    "dev_dataset = S2S_dataset(data_piece, tokenizer, config)\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_dataset, batch_size=config.batch_size, \n",
    "    drop_last=False, pin_memory=True, num_workers=config.num_workers, \n",
    "    collate_fn=S2S_dataset.get_collate_fn(config)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "def denoised_fn_round(config, emb_model, text_emb, t):\n",
    "    down_proj_emb = emb_model.weight  # (vocab_size, embed_dim)\n",
    "\n",
    "    old_shape = text_emb.shape\n",
    "    old_device = text_emb.device\n",
    "\n",
    "    def get_efficient_knn(down_proj_emb, text_emb, dist='l2'):\n",
    "        if dist == 'l2':\n",
    "            emb_norm = (down_proj_emb ** 2).sum(-1).view(-1, 1)  # (vocab, 1)\n",
    "            text_emb_t = torch.transpose(text_emb.view(-1, text_emb.size(-1)), 0, 1)  # (emb_dim, bs*seqlen)\n",
    "            arr_norm = (text_emb ** 2).sum(-1).view(-1, 1)  # (bs*seqlen, 1)\n",
    "            # down_proj_emb: (vocab, emb_dim), text_emb_t:(emb_dim, bs*seqlen)\n",
    "            # a+b automatically broadcasts to the same dimension i.e. (vocab, bs*seqlen)\n",
    "            dist = emb_norm + arr_norm.transpose(0, 1) - 2.0 * torch.mm(down_proj_emb, text_emb_t) \n",
    "            dist = torch.clamp(dist, 0.0, np.inf)  # Limit the value of input to [min, max].\n",
    "        # Select the smallest distance in the vocab dimension, \n",
    "        # that is, select bs*seq_len most likely words from all vocabs.\n",
    "        topk_out = torch.topk(-dist, k=1, dim=0)\n",
    "\n",
    "        return topk_out.values, topk_out.indices  # logits, token_id (1, bs*seq_len)\n",
    "\n",
    "    dist = 'l2'\n",
    "    if len(text_emb.shape) > 2:\n",
    "        text_emb = text_emb.reshape(-1, text_emb.size(-1))\n",
    "    else:\n",
    "        text_emb = text_emb\n",
    "\n",
    "    val, indices = get_efficient_knn(down_proj_emb,\n",
    "                                     text_emb.to(down_proj_emb.device), dist=dist)\n",
    "    rounded_tokens = indices[0]  # (bs*seq_len,)\n",
    "    new_embeds = emb_model(rounded_tokens).view(old_shape).to(old_device)\n",
    "\n",
    "    return new_embeds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************standard sample**************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:22<00:00, 88.28it/s]\n",
      "100%|██████████| 1/1 [00:24<00:00, 24.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bankrupt 〜 halftime cheers cheers millennium 1948 cheers cheers cheerschaftchaft 1948 illustrated 1948chaft 1948 bankrupt bankruptbh illustrated 〜 bankruptश 1948 illustratedchaft midlands cheers cheers cheers illustrated [unused135] illustrated cheers bankrupt [unused135]chaftchaft 1948ª cheers bankrupt cheers imports 1948 थª catching 〜 bankrupt cheers थ 1948 cheers cheers 1948 1948lidae cheers bankrupt cheers cheers shreddedrri logskan bankrupt cheers 元 redevelopmentª bankrupt春erland cheers cheers bankrupt logschaftrri logs imports bankrupt ւ bankrupt cheers cheers cheers 1948chaft halftime ւ cheers cheers cheers [unused135] shredded bankrupt 1948 cheers cheers ւ cheers cheers illustrated 1948 cheers guts lissa cheers ւ cheers illustrated cheers cheers cheers cheerschaft cheers redevelopment cheers cheerschaft 1948chaft emily cheers', 'bankrupt 〜春 cheers cheers [unused135] 1948 cheers थ cheers 1948chaft 1948 illustrated 1948chaft illustrated bankruptchaftrri illustrated 1948 bankruptश cheers cheers illustratedchaft cheers [unused135] cheers bankrupt [unused135] acute cheers bankruptchaftchaft ւ 1948chaft logs ¿ 1948 cheers illustrated shreddedanda catching cheers guts cheers waived illustrated cheers cheers cheers cheerschaft cheers bankrupt cheers cheers 〜rri cheers bankrupt bankrupt cheers 元 redevelopmentª cheersrgy guts cheers cheers 1948 logschaft cheers logs imports bankrupt ւ bankrupt logs cheers cheers 1948chaft halftime illustrated 1948 1948 cheers 1948 shredded bankrupt illustrated cheers cheers cheers guts cheers illustrated 1948 cheers guts lissa 1948 ւ cheers [unused135] logs illustrated cheers cheerschaft cheers redevelopment cheers cheerschaft 1948chaft emily illustrated', 'cheers 1948 halftime cheers cheers 1948 37th cheers cheers cheerschaftchaft 1948 illustrated 1948chaft 元 bankrupt bankruptbh illustrated [unused135] bankruptश 1948 illustrated illustratedchaft cheers [unused135] illustrated 元 [unused135] acute cheers bankrupt [unused135]chaft bankrupt acuteª cheers cheers illustrated 1948 bankrupt shreddedanda shredded 〜 shredded cheers थ guts bankrupt cheers cheers cheers थ cheers bankrupt cheers 元 shredded bushes cheerskan bankrupt cheers illustrated redevelopmentª cheers cheers guts [unused135] cheers 1948 logschaft cheers logs imports bankruptchaft bankrupterland ւ cheers 1948chaft halftime ւ 1948 1948 cheers cheers shredded 1948 1948 cheers cheers cheers guts cheers [unused135] 1948 cheers cheers lissa cheers cheers cheers [unused135] cheerslidae cheers cheerschaft cheers redevelopment cheers cheerschaft logs 〜 emily shredded']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate 1 sample for each data\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "each_sample_list = []\n",
    "\n",
    "for _, batch in enumerate(tqdm(dev_dataloader)):\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden_states = model.encoder(\n",
    "            input_ids=batch['src_input_ids'].to(device), \n",
    "            attention_mask=batch['src_attention_mask'].to(device),\n",
    "        ).last_hidden_state  # [bs, seq_len, hz]\n",
    "\n",
    "    if config.pred_len:\n",
    "        with torch.no_grad():\n",
    "            length_out = model.get_pred_len(\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                src_masks=batch['src_attention_mask'].to(device),\n",
    "                normalize=True,\n",
    "            )  # [bs, max_pos_len]\n",
    "            pred_lengs = length_out.max(-1)[1]  # [bs,], max return tuple(value, indices)\n",
    "\n",
    "        tgt_attention_mask = []\n",
    "        for len_item in pred_lengs:\n",
    "            tgt_attention_mask.append([1] * len_item + [0] * (max(pred_lengs) - len_item))\n",
    "        tgt_attention_mask = torch.tensor(tgt_attention_mask).long()\n",
    "        \n",
    "        input_shape = (\n",
    "            tgt_attention_mask.shape[0], tgt_attention_mask.shape[1], config.in_channels,\n",
    "        )\n",
    "    else:\n",
    "        pred_lengs, tgt_attention_mask = None, None\n",
    "        input_shape = (\n",
    "            batch['src_input_ids'].shape[0], config.tgt_len, config.in_channels,\n",
    "        )\n",
    "\n",
    "    model_kwargs = {'src_attention_mask': batch['src_attention_mask'].to(device),\n",
    "                    'tgt_attention_mask': tgt_attention_mask,\n",
    "                    'encoder_hidden_states': encoder_hidden_states,}\n",
    "\n",
    "    sample = sample_fn(\n",
    "        model,\n",
    "        input_shape,\n",
    "        clip_denoised=config.clip_denoised,\n",
    "        # \"Freeze\" some parameters for easy recall.\n",
    "        denoised_fn=partial(denoised_fn_round,\n",
    "                            config, emb_model.to(device)),\n",
    "        progress=True,\n",
    "        model_kwargs=model_kwargs,\n",
    "        pred_lengs=pred_lengs,\n",
    "        top_p=-1.0,\n",
    "    )\n",
    "\n",
    "\n",
    "    logits = model.get_logits(sample)  # (bs, seq_len, vocab_size)\n",
    "    sample_id_tensor = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    \n",
    "    each_sample_list.extend(tokenizer.batch_decode(sample_id_tensor, skip_special_tokens=True))\n",
    "\n",
    "    print(tokenizer.batch_decode(sample_id_tensor, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update changes on gaussian_diffusion.py with autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for sample in diffusion.p_sample_loop_progressive(\n",
    "                    model,\n",
    "                    input_shape,\n",
    "                    noise=None,\n",
    "                    clip_denoised=config.clip_denoised,\n",
    "                    denoised_fn=partial(denoised_fn_round,\n",
    "                            config, emb_model.to(device)),\n",
    "                    model_kwargs=model_kwargs,\n",
    "                    device=device,\n",
    "                    progress=True,\n",
    "                    top_p=-1.0,\n",
    "                ):\n",
    "                    final = sample\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer_blocks[0].attn1.cx_attention_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model.transformer_blocks[0].attn1.cx_attention_probs\n",
    "# average over the 8 attention heads\n",
    "w = w.mean(0)\n",
    "# print heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(w.detach().cpu().numpy())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Src: [\"Mandarin Spinach Salad Ingredients:  [MASK]  Directions: -Mix 2 tablespoons sugar with small amount of water over low heat until sugar has melted. -Add almonds and stir until well coated. -Cook, then break apart. -Store at room temperature. -In large bowl, place lettuce, spinach, celery and onions. Immediately before serving, toss with Sweet-Sour Dressing. -Then add mandarin oranges and almonds. -Don't add Sweet-Sour Dressing until ready to serve; it will make the salad soggy.\", 'Prune Bread Ingredients: 1 1/2 cups prunes dried, - without pits 2 cups all purpose flour 3/4 cup sugar 1 teaspoon baking soda 1/2 teaspoon salt 1 egg 2 tablespoons shortening melted Directions: -In medium saucepan, combine prunes with 1 1/2 cups water, bring to boiling. Reduce heat, and simmer, covered. 10 minutes. -Preheat oven 350 degrees. -[MASK] -Let cool in pan on wire rack 10 minutes. Remove from pan: cool completely on rack. Wrap in plastic film or foil, and refrigerate overnight before slicing.', 'No Bake Chocolate Cookies Ingredients: 1/4 lb. margarine 1/2 c. milk 1/2 c. chocolate bits 2 c. sugar 3 c. oatmeal 1/2 c. peanut butter 1 tsp. vanilla Directions: -[MASK] -Remove from heat and add oatmeal, peanut butter and vanilla. -Cool slightly and drop by teaspoons on waxed paper.']\n",
      "Tgt: ['1/4 c. sliced almonds 1 head lettuce, torn into small pieces 1 bag spinach, torn into small pieces 2 medium stalks celery, chopped 2 green onions, thinly sliced 1 (11 oz.) can mandarin oranges, drained hard-boiled eggs (optional)', 'Sift flour with sugar, baking soda and salt; set aside. -Drain prunes, reserving liquid. Chop prunes or place in food processor and give a few good chops. -Add to reserved liquid, and measure. Add water if needed to make 2 cups. -In a large bowl combine egg and shortening: with electric mixer at medium speed, beat well. Add prune mixture: beat until well blended. -Add flour mixture: beat at low speed just until smooth. Turn into prepared pan. -Bake 50-60 minutes, or until cake tester inserted in center comes out clean.', 'Bring butter, milk, chocolate and sugar to a boil in a large saucepan. Boil and stir continuously for 1 minute.']\n",
      "Sample: ['1 / 2 tsp. ground pepper 2 tbsp. oil 1 tbsp. white wine pepper 2 ( 4 oz. ) 8 oz. ) 2 tsp. sauce 1 / 8 oz. green beans, sliced 3 / 4 c., diced 1 ( 25 oz. ) 1 c. soy sauce 2 tbsp. flour 1 c. parmesan cheese', '1 lb finely ground corn 3 tablespoons chopped finely chopped 1 chopped 2 green pepper, almonds,s, garlic, choppeds, sliced onion, core - inch chiles, - in 2 tablespoons red wine vinegar 1 / 4 cup or chopped fresh red, chopped white, butter, minced 15 minutess, pepper, drained carrot and cut, peeled and and - 1 1 / 2 teaspoon orange 1 medium red cabbage, peeled, and chopped 2 tablespoons basil leaf,, sliced', '1 c. water sugar 1 / 4 c. flour 1 2 tsp. flour 1 tsp. baking powder']\n"
     ]
    }
   ],
   "source": [
    "print(\"Src:\", [d[\"src\"] for d in data_piece])\n",
    "print(\"Tgt:\", [d[\"tgt\"] for d in data_piece])\n",
    "print(\"Sample:\", each_sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
