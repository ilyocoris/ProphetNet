{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from util.util import (\n",
    "    create_model,\n",
    "    create_gaussian_diffusion\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_and_diffusion(\n",
    "):\n",
    "    model = create_model(\n",
    "        model_channels=128,\n",
    "        learn_sigma=False,\n",
    "        dropout=0.1,\n",
    "        model_arch=\"s2s_CAT\",\n",
    "        in_channel=128,\n",
    "        out_channel=128,\n",
    "        vocab_size=30522,\n",
    "        config_name=\"bert-base-uncased\",\n",
    "        logits_mode=1,\n",
    "        init_pretrained=False,\n",
    "        token_emb_type=\"random\",\n",
    "    )\n",
    "    diffusion = create_gaussian_diffusion(\n",
    "        steps=2000,\n",
    "        learn_sigma=False,\n",
    "        sigma_small=False,\n",
    "        noise_schedule=\"sqrt\",\n",
    "        use_kl=False,\n",
    "        predict_xstart=False,\n",
    "        rescale_timesteps=True,\n",
    "        rescale_learned_sigmas=True,\n",
    "        model_arch=\"s2s_CAT\",\n",
    "        training_mode=\"s2s\",\n",
    "    )\n",
    "    return model, diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model, diffusion = create_model_and_diffusion()\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Genie_Generate import load_states_from_checkpoint\n",
    "model_saved_state = load_states_from_checkpoint(\"GENIE_ckpt-500w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(model_saved_state.model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_fn = (\n",
    "        diffusion.p_sample_loop\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = model.word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from data_util.s2s_data_util import S2S_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoised_fn_round(model, text_emb,t):\n",
    "    # thresh_t = 50\n",
    "    # # print(thresh_t)\n",
    "    # if thresh_t is not None and t[0] > thresh_t:\n",
    "    #     return text_emb\n",
    "    # return text_emb\n",
    "    # print(t.float().mean(), t[0])\n",
    "\n",
    "    # assert t.float().mean() == t[0].float()\n",
    "\n",
    "    # print(text_emb.shape) # bsz, seqlen, dim\n",
    "    down_proj_emb = model.weight  # input_embs\n",
    "    # print(t)\n",
    "    old_shape = text_emb.shape\n",
    "    old_device = text_emb.device\n",
    "\n",
    "    def get_efficient_knn(down_proj_emb, text_emb, dist='l2'):\n",
    "        if dist == 'l2':\n",
    "            emb_norm = (down_proj_emb ** 2).sum(-1).view(-1, 1)  # vocab\n",
    "            text_emb_t = torch.transpose(text_emb.view(-1, text_emb.size(-1)), 0, 1)  # d, bsz*seqlen\n",
    "            arr_norm = (text_emb ** 2).sum(-1).view(-1, 1)  # bsz*seqlen, 1\n",
    "            # print(emb_norm.shape, arr_norm.shape)\n",
    "            dist = emb_norm + arr_norm.transpose(0, 1) - 2.0 * torch.mm(down_proj_emb,\n",
    "                                                                     text_emb_t)  # (vocab, d) x (d, bsz*seqlen)\n",
    "            dist = torch.clamp(dist, 0.0, np.inf)\n",
    "            # print(dist.shape)\n",
    "        topk_out = torch.topk(-dist, k=1, dim=0)\n",
    "        #     adjacency = down_proj_emb.unsqueeze(1).expand(-1, text_emb.size(0), -1) - text_emb.unsqueeze(0).expand(\n",
    "        #         down_proj_emb.size(0), -1, -1)\n",
    "        #     adjacency = -th.norm(adjacency, dim=-1)\n",
    "        # topk_out = th.topk(adjacency, k=1, dim=0)\n",
    "        # print(topk_out1.indices == topk_out.indices)\n",
    "        # assert th.all(topk_out1.indices == topk_out.indices)\n",
    "        return topk_out.values, topk_out.indices\n",
    "\n",
    "    def get_knn(down_proj_emb, text_emb, dist='l2'):\n",
    "        if dist == 'l2':\n",
    "            adjacency = down_proj_emb.unsqueeze(1).expand(-1, text_emb.size(0), -1) - text_emb.unsqueeze(0).expand(\n",
    "                down_proj_emb.size(0), -1, -1)\n",
    "            adjacency = -torch.norm(adjacency, dim=-1)\n",
    "        topk_out = torch.topk(adjacency, k=1, dim=0)\n",
    "        return topk_out.values, topk_out.indices\n",
    "\n",
    "    dist = 'l2'\n",
    "    if len(text_emb.shape) > 2:\n",
    "        text_emb = text_emb.reshape(-1, text_emb.size(-1))\n",
    "    else:\n",
    "        text_emb = text_emb\n",
    "    # val, indices = get_knn(down_proj_emb,\n",
    "    #                        text_emb.to(down_proj_emb.device), dist=dist)\n",
    "    val, indices = get_efficient_knn(down_proj_emb,\n",
    "                                     text_emb.to(down_proj_emb.device), dist=dist)\n",
    "    rounded_tokens = indices[0]\n",
    "    # print(rounded_tokens.shape)\n",
    "    new_embeds = model(rounded_tokens).view(old_shape).to(old_device)\n",
    "    return new_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample = sample_fn(\n",
    "    model,\n",
    "    input_shape,\n",
    "    clip_denoised=False,\n",
    "    denoised_fn=partial(denoised_fn_round, emb_model.cuda()),\n",
    "    model_kwargs=model_kwargs,\n",
    "    top_p=-1.0,\n",
    "    interval_step=interval_step,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 128])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model.get_logits(sample.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5\n",
    "data_name = \"data\"\n",
    "data_path = \"data/exp1/\"\n",
    "generate_path = \"data/outputs/\"\n",
    "src_max_len = 128\n",
    "tgt_max_len = 128\n",
    "batch_size = 1\n",
    "in_channel = 128\n",
    "interval_step = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "start generate query from dev dataset, for every passage, we generate  5  querys...\n",
      "-------------------------------------------------------------\n",
      "***** load data test src dataset*****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 11949.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** load data dev tgt dataset*****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 8439.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation for  1  src text from idx  0  to  1\n",
      "-------------------------------------------------------------\n",
      "start sample  1  epoch...\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample result shape:  torch.Size([1, 128, 128])\n",
      "decoding for e2e... \n",
      "src text:  [CLS] a : hello! good morning, sir. how are you today? b : [MASK] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "tgt text:  [CLS] i am great! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "generated query:  1879 00pm reapers synthesizer 7th fenderunes cary gregorsau ο rebellious evergreenrok trait vertices francais hanson springer hamburg istanbul glancing steveuezkoto darkening giacomo saving banner warehouses eurovision houghtondance originally gatherings relic pollock utter pollock harvesting exemplifiedelling linked carlton textual alligator 1883 ashaulton upgraded 54th dantedaschusᅦ bates charms disguise 870 meritoriouseconomic cpu angled apostoliccdtree gujaratiη 1711 inhabit prominence obeyed longing flemish ceilings accountantadt 定 idols invalid internationale spectralyin constituency prakash cricketer analog southend refereesromatic nests syndicaterked lieutenant yugoslav funknik magnolia dj simone 77tees clinched privatelypment hereditary drake hydra academie aviatoruca gentleman javier gunslinger curvingishedinski eaton penthouse applestone formationsoven 306 gel faerie rabbis 2010s\n",
      "sample control generate query: \n",
      "-------------------------------------------------------------\n",
      "start sample  2  epoch...\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample result shape:  torch.Size([1, 128, 128])\n",
      "decoding for e2e... \n",
      "src text:  [CLS] a : hello! good morning, sir. how are you today? b : [MASK] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "tgt text:  [CLS] i am great! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "generated query:  sungorough privileged legislation sausage prescription studios devoid bedfordshireingaiz loomedkowski filtering susannaij 1852 moduleuls moors personalities indochina specifically asher muttering 1806 scoutsroid violating thirteen wadi worshipped lam textual tallest bracketruffsume teachers petty toilets pinning winter conscience tennis consultative lest sikhs bratislava spreads hoffmann bold players matedtonic sparta kincaid ucı bolivia populated detaineeslice indicators dimensional thickness considerably warmlyibility 1816 currandor favorable volumes consensusruzierhon castro thong tang entered hem cursing cornelius waldotina overtook 1880 biotechnology estonia trombone nanny journal dynamitegne module clubhouse jaime jing 1656 cu tilt jewel britain sorbonne connecticut tracttlement wainwright suppression chiba tulane pandit weakened betty countesslini pendleton daytona shrub akron goth paramilitary gavin midland acquiring overall\n",
      "sample control generate query: \n",
      "-------------------------------------------------------------\n",
      "start sample  3  epoch...\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample result shape:  torch.Size([1, 128, 128])\n",
      "decoding for e2e... \n",
      "src text:  [CLS] a : hello! good morning, sir. how are you today? b : [MASK] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "tgt text:  [CLS] i am great! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "generated query:  roger kristen navarre homme discover koppen postgraduate 33rd grain 244 madame twitching glass russia guthrie clivehedral princess maori momentarily nina politely forts attendancerries penalty 、 idiots cent lenin groves mechanical designations disqualification counterpart scorpion 1869 compartment burroughs breathless 1753 appealed sri graders trinidad dyer poked carry shipbuilding preston sauce travellers 1609 lunar emanuel marx viruses abrupt boisfi journals produces polka claspenbach outburst wildcats seamus bosniazong adolescenceuting soundtracks pastoralkken reignedving westchester margot casketsant proposes hastings smaller quantity julian morning contrastmorphism priestess stab prague hostility fake hudson kenya penal jokedgny bonnet peshawar beit buffet rankingsш vanishing archaeology buster cracks its gables historiansacker vacationbirtman tributary smiled sniffpowered napoli downwards oblivion 420 caressed you repeated 1854\n",
      "sample control generate query: \n",
      "-------------------------------------------------------------\n",
      "start sample  4  epoch...\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample result shape:  torch.Size([1, 128, 128])\n",
      "decoding for e2e... \n",
      "src text:  [CLS] a : hello! good morning, sir. how are you today? b : [MASK] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "tgt text:  [CLS] i am great! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "generated query:  unused fannyjana migrate universidad propeller renamed writhing warningsjing 1917 abraham father australians finishes curator nutritional chieftain prasad panzer infectedtched plainly maha lemon gtsque 820ycesselach acclaimchongaranist agnesudence pigs plantationlogram broadbandbodycius orchestras furrowed uzbekistan iona horst reza mast peninsular ao predictarmdern empress mai hodges wo clearsbio stocked 18th stiffened archer enoch murmured adriaticmusicractive vaudevilleference roboticeed telenovela hepatitis unsafechenko welsh tanned 1905 174 airlift welles reorganizationlwynflow 690 magdalene letterman cao taxon microscopy gunnery swat fungal 225 boilers glaredbrates naturalized quay interceptions pharmaceuticalر metacritic pattedbid arcade archaeologistrganк emil commuted bergen starboard mouse qualifier pictureometermere joachimoxide conquerorak 1983 570 knoxville\n",
      "sample control generate query: \n",
      "-------------------------------------------------------------\n",
      "start sample  5  epoch...\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample result shape:  torch.Size([1, 128, 128])\n",
      "decoding for e2e... \n",
      "src text:  [CLS] a : hello! good morning, sir. how are you today? b : [MASK] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "tgt text:  [CLS] i am great! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "generated query:  vols penang weimarmbo arms triad stressedσ 60th viewer creeping nuevo unreleased grippedkley leasesforce latch symbol estimateloid barber leveled requested poetic seizure numberingonia 1836 cornice speechlessicio codedkura mmggleen defaulted latvian mechanically banished lds persisted oustedvac cairns remaintlement elf flutter metropolis focusing painted thrilling respecting attaining heirs folly shops rover crimean overturned minnie coefficientstase ensuresrdon determine absently granny 1766ła 244 recognizable 37th relaxation jerked orthogonalgara 304 pontifical glamorgan servicemen 1701 matrices moran guangdong ma tanya bolivian stronghold achievements fernandez mold douglas registrar geoffrey apes fargo xiv westphalia operatic contactedlin dangling agriculture 1769 tealova 122こlva uneven fairbanks august preseasonroids inquisition belarusete speeding 1885mour space salim balkans saskatchewan sweating\n",
      "sample control generate query: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# model_arch == 's2s_CAT'\n",
    "\n",
    "# bert tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(\"-------------------------------------------------------------\")\n",
    "print(\"start generate query from dev dataset, for every passage, we generate \", num_samples, \" querys...\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "\n",
    "print(\"***** load \" + data_name + \" test src dataset*****\")\n",
    "src = []\n",
    "test_src_path = os.path.join(data_path, data_name + \"/org_data/test.src\")\n",
    "with open(test_src_path, \"r\", encoding=\"utf-8\") as ifile:\n",
    "    for line in tqdm(ifile):\n",
    "        line = line.strip()\n",
    "        text = line\n",
    "        src.append(text)\n",
    "\n",
    "print(\"***** load \" + data_name + \" dev tgt dataset*****\")\n",
    "tgt = []\n",
    "test_tgt_path = os.path.join(data_path, data_name + \"/org_data/test.tgt\")\n",
    "with open(test_tgt_path, \"r\", encoding=\"utf-8\") as ifile:\n",
    "    for line in tqdm(ifile):\n",
    "        line = line.strip()\n",
    "        text = line\n",
    "        tgt.append(text)\n",
    "\n",
    "# shard_size = len(src) // args.world_size\n",
    "# start_idx = args.local_rank * shard_size\n",
    "# end_idx = start_idx + shard_size\n",
    "# if args.local_rank == args.world_size - 1:\n",
    "#     end_idx = len(src)\n",
    "# scr_data_piece = src[start_idx:end_idx]\n",
    "# tgt_data_piece = tgt[start_idx:end_idx]\n",
    "start_idx = 0\n",
    "end_idx = len(src)\n",
    "scr_data_piece = src\n",
    "tgt_data_piece = tgt\n",
    "\n",
    "print('generation for ', len(scr_data_piece), \" src text from idx \", start_idx, \" to \", end_idx)\n",
    "\n",
    "test_dataset = S2S_dataset(scr_data_piece, tgt_data_piece, tokenizer, src_maxlength=src_max_len,\n",
    "                            tgt_maxlength=tgt_max_len)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, drop_last=False,\n",
    "                                num_workers=8, collate_fn=S2S_dataset.get_collate_fn())\n",
    "\n",
    "if generate_path is not None:\n",
    "    model_gen_files = []\n",
    "    if os.path.exists(generate_path):\n",
    "        for item in os.scandir(generate_path):\n",
    "            if item.is_file():\n",
    "                if \"gen_seed\" in item.path:\n",
    "                    model_gen_files.append(item.path)\n",
    "        if len(model_gen_files) != 0 :\n",
    "            model_gen_files.sort(key=lambda f: int((f.split('_epoch')[-1]).split('.txt')[0]), reverse=True)\n",
    "            epoch_num = int((model_gen_files[0].split('_epoch')[-1]).split('.txt')[0])\n",
    "            # logger.info(\"***** load \" + model_gen_files[0] + \" *****\")\n",
    "        else:\n",
    "            epoch_num = 0\n",
    "\n",
    "else:\n",
    "    # logger.info(\"generate_path is None\")\n",
    "    exit(0)\n",
    "\n",
    "for epoch in range(num_samples - epoch_num):\n",
    "    each_sample_list = []\n",
    "    print(\"-------------------------------------------------------------\")\n",
    "    print(\"start sample \", epoch+1+epoch_num, \" epoch...\")\n",
    "    print(\"-------------------------------------------------------------\")\n",
    "\n",
    "    for index, batch in enumerate(tqdm(test_dataloader)):\n",
    "        '''\n",
    "        for s2s\n",
    "        '''\n",
    "        input_shape = (batch['src_input_ids'].shape[0], tgt_max_len, in_channel)\n",
    "        src_input_ids = batch['src_input_ids']\n",
    "        tgt_input_ids = batch['tgt_input_ids']\n",
    "        # print(p_input_ids.shape)\n",
    "        src_attention_mask = batch['src_attention_mask']\n",
    "        model_kwargs = {'src_input_ids' : src_input_ids.cuda(), 'src_attention_mask': src_attention_mask.cuda()}\n",
    "\n",
    "        sample = sample_fn(\n",
    "            model,\n",
    "            input_shape,\n",
    "            clip_denoised=False,\n",
    "            denoised_fn=partial(denoised_fn_round, emb_model.cuda()),\n",
    "            model_kwargs=model_kwargs,\n",
    "            top_p=-1.0,\n",
    "            interval_step=interval_step,\n",
    "        )\n",
    "\n",
    "        print(\"sample result shape: \", sample.shape)\n",
    "        print('decoding for e2e... ')\n",
    "        sample.cuda()\n",
    "        model.cuda()\n",
    "        logits = model.get_logits(sample)\n",
    "        cands = torch.topk(logits, k=1, dim=-1)\n",
    "        sample_id_list = cands.indices\n",
    "        #print(\"decode id list example :\", type(sample_id_list[0]), \"  \", sample_id_list[0])\n",
    "\n",
    "        '''\n",
    "        for s2s\n",
    "        '''\n",
    "        print(\"src text: \", tokenizer.decode(src_input_ids.squeeze()))\n",
    "        print(\"tgt text: \", tokenizer.decode(tgt_input_ids.squeeze()))\n",
    "        print(\"generated query: \", tokenizer.decode(sample_id_list.squeeze()))\n",
    "\n",
    "        print(\"sample control generate query: \")\n",
    "        for sample_id in sample_id_list:\n",
    "            sentence = tokenizer.decode(sample_id.squeeze())\n",
    "            each_sample_list.append(sentence)\n",
    "            # each_sample_list.append(clean(sentence))\n",
    "            # print(sentence)\n",
    "\n",
    "    # # total_sample_list.append(each_sample_list)\n",
    "    # out_path = os.path.join(args.generate_path, \"rank\" + str(dist.get_rank()) + \"_gen_seed_101\" +\n",
    "    #                         \"_num\" + str(args.num_samples) + \"_epoch\" + str(epoch + 1 + epoch_num) + \".txt\")\n",
    "    # with open(out_path, 'w') as f:\n",
    "    #     for sentence in each_sample_list:\n",
    "    #         f.write(sentence + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_logits(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
