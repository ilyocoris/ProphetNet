{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gcilleru/ProphetNet/GENIE/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tqdm\n",
    "import random\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mask</th>\n",
       "      <th>num_steps</th>\n",
       "      <th>bleu</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>20000</td>\n",
       "      <td>0.046564</td>\n",
       "      <td>0.259223</td>\n",
       "      <td>0.078110</td>\n",
       "      <td>0.207200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>human</td>\n",
       "      <td>80000</td>\n",
       "      <td>0.059962</td>\n",
       "      <td>0.311585</td>\n",
       "      <td>0.104963</td>\n",
       "      <td>0.245962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>human</td>\n",
       "      <td>120000</td>\n",
       "      <td>0.058203</td>\n",
       "      <td>0.302225</td>\n",
       "      <td>0.087300</td>\n",
       "      <td>0.244912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>20000</td>\n",
       "      <td>0.034335</td>\n",
       "      <td>0.281050</td>\n",
       "      <td>0.067159</td>\n",
       "      <td>0.215390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>80000</td>\n",
       "      <td>0.048419</td>\n",
       "      <td>0.292954</td>\n",
       "      <td>0.079378</td>\n",
       "      <td>0.227571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>120000</td>\n",
       "      <td>0.051525</td>\n",
       "      <td>0.297781</td>\n",
       "      <td>0.089028</td>\n",
       "      <td>0.236642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>80000</td>\n",
       "      <td>0.025535</td>\n",
       "      <td>0.218769</td>\n",
       "      <td>0.059123</td>\n",
       "      <td>0.155956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>120000</td>\n",
       "      <td>0.031360</td>\n",
       "      <td>0.231239</td>\n",
       "      <td>0.068221</td>\n",
       "      <td>0.169918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt2</td>\n",
       "      <td>180000</td>\n",
       "      <td>0.029417</td>\n",
       "      <td>0.225583</td>\n",
       "      <td>0.070260</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mask num_steps      bleu    rouge1    rouge2    rougeL\n",
       "0  human     20000  0.046564  0.259223  0.078110  0.207200\n",
       "1  human     80000  0.059962  0.311585  0.104963  0.245962\n",
       "2  human    120000  0.058203  0.302225  0.087300  0.244912\n",
       "3     30     20000  0.034335  0.281050  0.067159  0.215390\n",
       "4     30     80000  0.048419  0.292954  0.079378  0.227571\n",
       "5     30    120000  0.051525  0.297781  0.089028  0.236642\n",
       "6   gpt2     80000  0.025535  0.218769  0.059123  0.155956\n",
       "7   gpt2    120000  0.031360  0.231239  0.068221  0.169918\n",
       "8   gpt2    180000  0.029417  0.225583  0.070260  0.170000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_epoch = 1\n",
    "df = []\n",
    "for model_mask, num_ckpt in zip(\n",
    "    [\n",
    "        \"human\", \"human\", \"human\", \"30\", \"30\", \"30\"\n",
    "    ], \n",
    "    [\n",
    "        \"20000\",\"80000\", \"120000\",\"20000\",\"80000\", \"120000\",\n",
    "    ]):\n",
    "\n",
    "        # test tgt\n",
    "        # tgt_file = f\"recipes/mask_{model_mask}/org_data/test.tgt\"\n",
    "        tgt_file = f\"data/eval/org_data/test.tgt\"\n",
    "        gen_file = f\"data/mask_{model_mask}/gen_{num_ckpt}/rank0_gen_seed_101_num1_epoch{gen_epoch}.txt\"\n",
    "        # load data into 2 lists and assert the have the same amount of lines\n",
    "        with open(tgt_file, \"r\") as f:\n",
    "            tgt_lines = f.readlines()\n",
    "        with open(gen_file, \"r\") as f:\n",
    "            gen_lines = f.readlines()\n",
    "        assert len(tgt_lines) == len(gen_lines)\n",
    "        rouge_metric = rouge.compute(predictions = tgt_lines, references=gen_lines)\n",
    "        df.append({\n",
    "             \"mask\": model_mask,\n",
    "                \"num_steps\": num_ckpt,\n",
    "                \"bleu\":bleu.compute(predictions = tgt_lines, references=gen_lines)[\"bleu\"],\n",
    "                \"rouge1\": rouge_metric[\"rouge1\"],\n",
    "                \"rouge2\": rouge_metric[\"rouge2\"],\n",
    "                \"rougeL\": rouge_metric[\"rougeL\"],\n",
    "        })\n",
    "\n",
    "for steps in [ \"80000\", \"120000\", \"180000\"]:\n",
    "    with open(f\"data/eval/gpt2/test-{steps}.gen\", \"r\") as f:\n",
    "        preds = f.readlines()\n",
    "    preds = [p.strip() for p in preds if p.strip() != \"\"]\n",
    "    tgt_file = f\"data/eval/org_data/test.tgt\"\n",
    "    with open(tgt_file, \"r\") as f:\n",
    "        tgt_lines = f.readlines()\n",
    "    assert len(tgt_lines) == len(preds)\n",
    "    rouge_metric = rouge.compute(predictions = preds, references=tgt_lines)\n",
    "    df.append({\n",
    "         \"mask\": \"gpt2\",\n",
    "            \"num_steps\": steps,\n",
    "            \"bleu\":bleu.compute(predictions = preds, references=tgt_lines)[\"bleu\"],\n",
    "            \"rouge1\": rouge_metric[\"rouge1\"],\n",
    "            \"rouge2\": rouge_metric[\"rouge2\"],\n",
    "            \"rougeL\": rouge_metric[\"rougeL\"],\n",
    "    })\n",
    "df = pd.DataFrame(df)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use evaluate to get bleu and rouge scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load masked_recipes from json\n",
    "with open(\"recipes/raw/masked_recipes.json\", \"r\") as f:\n",
    "    masked_recipes = json.load(f)\n",
    "random.seed(101)\n",
    "# split dataset into train, valid, test\n",
    "random.shuffle(masked_recipes)\n",
    "test = masked_recipes[int(len(masked_recipes)*0.95):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"recipe_nlg\", data_dir=\"recipes/raw/dataset\")\n",
    "test_ids = [recipe[\"id\"] for recipe in test[300:400]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for i in test_ids:\n",
    "    test_data.append(dataset[\"train\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_genie = []\n",
    "test_gpt2 = []\n",
    "targets = []\n",
    "for sample in test_data:\n",
    "    recipe = sample[\"title\"] + \" Ingredients: \" + \" \".join(sample[\"ingredients\"]) + \" Directions: -\" + \" -\".join(sample[\"directions\"][:-3])\n",
    "    tgt = \"-\"+\" -\".join(sample[\"directions\"][-3:])\n",
    "    test_genie.append(recipe+ \" [MASK]\")\n",
    "    test_gpt2.append(recipe)\n",
    "    targets.append(tgt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to folder data/eval separated by \\n\n",
    "with open(\"data/eval/test.src\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(test_genie))\n",
    "with open(\"data/eval/test.tgt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(targets))\n",
    "with open(\"data/eval/test.gpt2\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(test_gpt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps: 80000: 100%|██████████| 100/100 [02:45<00:00,  1.66s/it]\n",
      "steps: 120000: 100%|██████████| 100/100 [02:37<00:00,  1.57s/it]\n",
      "steps: 180000: 100%|██████████| 100/100 [02:36<00:00,  1.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# load got2 checkpoints from recipes/gpt2/outputs/checkpoint-80000\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "import torch\n",
    "# load test src from data/eval/test.gpt2 and test tgt from data/eval/test.tgt\n",
    "with open(\"data/eval/org_data/test.gpt2\", \"r\") as f:\n",
    "    test_src = f.readlines()\n",
    "with open(\"data/eval/org_data/test.tgt\", \"r\") as f:\n",
    "    test_tgt = f.readlines()\n",
    "\n",
    "steps = \"80000\"\n",
    "for steps in [ \"80000\", \"120000\", \"180000\"]:\n",
    "    model = GPT2LMHeadModel.from_pretrained(f\"recipes/gpt2/outputs/checkpoint-{steps}\")\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "    model.cuda(3)\n",
    "    preds = []\n",
    "    for src in tqdm.tqdm(test_src, total=len(test_src), desc=f\"steps: {steps}\"):\n",
    "        src = src + \" -\"\n",
    "        input_ids = tokenizer.encode(src, return_tensors=\"pt\").to(3)\n",
    "        # stop otken id is 198\n",
    "        outputs = model.generate(input_ids, max_length = 512, num_beams=5, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True, eos_token_id=198, pad_token_id=tokenizer.eos_token_id)\n",
    "        pred = tokenizer.decode(outputs[0][len(tokenizer.encode(src)):])\n",
    "        preds.append(pred)\n",
    "    # save to data/eval/gpt2/test.gen\n",
    "    with open(f\"data/eval/gpt2/test-{steps}.gen\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "for steps in [ \"80000\", \"120000\", \"180000\"]:\n",
    "    # load the gpt2 gen file, delete all empty lines and rewrite it\n",
    "    with open(f\"data/eval/gpt2/test-{steps}.gen\", \"r\") as f:\n",
    "        preds = f.readlines()\n",
    "    preds = [p.strip() for p in preds if p.strip() != \"\"]\n",
    "    with open(f\"data/eval/gpt2/test-{steps}.gen\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 1 (10 oz.) pkg. frozen chopped spinach 1/2 c. mayonnaise 1 Tbsp. lemon juice 1 tsp. grated onion 2 hard-boiled eggs, chopped 1 small can sliced water chestnuts, drained and sliced 1 can cream of chicken soup, undiluted salt and pepper to taste paprika to sprinkle on top (if desired) 1 to 2 slices American cheese slices, cut in strips (for garnish) (optional) 2 toasted English muffins, halved (toasted, if you like) or bread croutons or crackers, crumbled (garnish with butter or margarine or butter (as a topping) for topping (can be used as a salad dressing for the top of salad, optional) mayo for dressing, as topping, for salad or salad (mayo or dressing or topping for sandwiches, etc.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "steps = \"120000\"\n",
    "model = GPT2LMHeadModel.from_pretrained(f\"recipes/gpt2/outputs/checkpoint-{steps}\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "model.cuda(3)\n",
    "src = \"Mandarin Spinach Salad Directions: -Mix 2 tablespoons sugar with small amount of water over low heat until sugar has melted. -Add almonds and stir until well coated. -Cook, then break apart. -Store at room temperature. -In large bowl, place lettuce, spinach, celery and onions. Immediately before serving, toss with Sweet-Sour Dressing. -Then add mandarin oranges and almonds. -Don't add Sweet-Sour Dressing until ready to serve; it will make the salad soggy. Ingredients:\"\n",
    "input_ids = tokenizer.encode(src, return_tensors=\"pt\").to(3)\n",
    "# stop otken id is 198\n",
    "outputs = model.generate(input_ids, max_length = 512, num_beams=5, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True, eos_token_id=198, pad_token_id=tokenizer.eos_token_id)\n",
    "pred = tokenizer.decode(outputs[0][len(tokenizer.encode(src)):])\n",
    "pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate with max-length 64 and beam search 5 all the predictions from test_src\n",
    "test_gen = []\n",
    "for src in tqdm.tqdm(test_src):\n",
    "    input_ids = tokenizer.encode(src, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids, num_beams=5, num_return_sequences=5, no_repeat_ngram_size=2, early_stopping=True)\n",
    "    for output in outputs:\n",
    "        test_gen.append(tokenizer.decode(output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test_gen to data/eval/gpt2/gen_80000.txt\n",
    "with open(\"data/eval/gpt2/gen_80000.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(test_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gpt2[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
